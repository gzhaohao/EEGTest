{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gzhaohao/EEGTest/blob/main/bcieeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **获取数据并解压**"
      ],
      "metadata": {
        "id": "Y_5mZ-dPoeWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.bbci.de/competition/download/competition_iv/BCICIV_2a_gdf.zip\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat\n",
        "!wget https://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat\n",
        "!unzip -q BCICIV_2a_gdf.zip\n",
        "!rm BCICIV_2a_gdf.zip"
      ],
      "metadata": {
        "id": "qo7_FvGtnYgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **挂载Google云盘**"
      ],
      "metadata": {
        "id": "umVk4TmVf_bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KdxvkPcOfVlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **安装python依赖**"
      ],
      "metadata": {
        "id": "_xHR_RP8AcyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne\n",
        "!pip install ipdb"
      ],
      "metadata": {
        "id": "A0GfDYvmAj_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model.py**"
      ],
      "metadata": {
        "id": "cCoqY8yp31eh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez0GDqI10HEy"
      },
      "outputs": [],
      "source": [
        "# 导入必要的库\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "# 按照论文创建EEGNet\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, n_classes=4, channels=22, samples=1000,\n",
        "                 dropout_rate=0.5, kernel_length1=64, kernel_length2=16,\n",
        "                 f1=8, d=2, f2=16):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.f1 = f1\n",
        "        self.f2 = f2\n",
        "        self.d = d\n",
        "        self.samples = samples\n",
        "        self.n_classes = n_classes\n",
        "        self.channels = channels\n",
        "        self.kernel_length1 = kernel_length1\n",
        "        self.kernel_length2 = kernel_length2\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        block1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=self.f1,\n",
        "                kernel_size=(1, self.kernel_length1),\n",
        "                stride=1,\n",
        "                padding=(0, self.kernel_length1//2),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f1)\n",
        "        )\n",
        "\n",
        "        block2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f1,\n",
        "                out_channels=self.f1*self.d,\n",
        "                kernel_size=(self.channels, 1),\n",
        "                groups=self.f1,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f1*self.d),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(\n",
        "                kernel_size=(1, 4),\n",
        "                stride=4\n",
        "            ),\n",
        "            nn.Dropout(p=self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        block3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f2,\n",
        "                out_channels=self.f2,\n",
        "                kernel_size=(1, self.kernel_length2),\n",
        "                stride=1,\n",
        "                padding=(0, self.kernel_length2//2),\n",
        "                groups=self.f1*self.d,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.f1*self.d,\n",
        "                out_channels=self.f2,\n",
        "                kernel_size=(1, 1),\n",
        "                stride=1,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=self.f2),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(\n",
        "                kernel_size=(1, 8),\n",
        "                stride=8\n",
        "            ),\n",
        "            nn.Dropout(p=self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.EEGNetLayer = nn.Sequential(block1, block2, block3)\n",
        "\n",
        "        self.ClassifierBlock = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=self.f2*round(round(self.samples//4)//8),\n",
        "                out_features=self.n_classes,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) != 4:\n",
        "            x = torch.unsqueeze(x, dim=1)\n",
        "        x = self.EEGNetLayer(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.ClassifierBlock(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 模型结构可视化\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = EEGNet().to(device)\n",
        "    print(summary(model, input_size=(1, 22, 1000)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model_train.py**"
      ],
      "metadata": {
        "id": "FEh-At_B4JEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import mne\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import copy\n",
        "import time\n",
        "#from model import EEGNet\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 1、创建必要的本地目录，用于保存数据\n",
        "if not os.path.exists('2a_train_pre'):\n",
        "    os.makedirs('2a_train_pre')\n",
        "\n",
        "# 2、原始数据读取和通道重命名\n",
        "data_path = ['A0'+str(i)+'T' for i in range(1, 10)]\n",
        "raw = [mne.io.read_raw_gdf(input_fname='./drive/MyDrive/'+path+'.gdf',\n",
        "                           stim_channel=\"auto\",\n",
        "                           preload=True,\n",
        "                           verbose=\"error\") for path in data_path]\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    raw[i].rename_channels({'EEG-Fz': 'Fz', 'EEG-0': 'FC3', 'EEG-1': 'FC1', 'EEG-2': 'FCz', 'EEG-3': 'FC2',\n",
        "                            'EEG-4': 'FC4', 'EEG-5': 'C5', 'EEG-C3': 'C3', 'EEG-6': 'C1', 'EEG-Cz': 'Cz',\n",
        "                            'EEG-7': 'C2', 'EEG-C4': 'C4', 'EEG-8': 'C6', 'EEG-9': 'CP3', 'EEG-10': 'CP1',\n",
        "                            'EEG-11': 'CPz', 'EEG-12': 'CP2', 'EEG-13': 'CP4', 'EEG-14': 'P1', 'EEG-15': 'Pz',\n",
        "                            'EEG-16': 'P2', 'EEG-Pz': 'POz'})\n",
        "\n",
        "# 3、提取MI时间，完成坏值清洗，并封装\n",
        "events = []\n",
        "event_ids = []\n",
        "for i in range(len(raw)):\n",
        "    event_to_id = dict({'769': 7, '770': 8, '771': 9, '772': 10})\n",
        "    if i == 3:\n",
        "        event_to_id = dict({'769': 5, '770': 6, '771': 7, '772': 8})\n",
        "        event, _ = mne.events_from_annotations(raw[i], verbose=False)\n",
        "        events.append(event)\n",
        "        ids = np.unique(events[i][:, 2])\n",
        "        event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "        event_ids.append(event_id)\n",
        "        raw[i].load_data()\n",
        "        data = raw[i].get_data()\n",
        "    else:\n",
        "        event, _ = mne.events_from_annotations(raw[i], verbose=False)\n",
        "        events.append(event)\n",
        "        ids = np.unique(events[i][:, 2])\n",
        "        event_id = {k: v for k, v in event_to_id.items() if v in ids}\n",
        "        event_ids.append(event_id)\n",
        "        raw[i].load_data()\n",
        "        data = raw[i].get_data()\n",
        "    for i_chan in range(data.shape[0]):\n",
        "        chan = data[i_chan]\n",
        "        data[i_chan] = np.where(chan == np.min(chan), np.nan, chan)\n",
        "        mask = np.isnan(data[i_chan])\n",
        "        chan_mean = np.nanmean(data[i_chan])\n",
        "        data[i_chan, mask] = chan_mean\n",
        "    raw[i] = mne.io.RawArray(data, raw[i].info, verbose=\"error\")\n",
        "\n",
        "# 4、切段、去EOG、做标准化，封存数据为npz\n",
        "tmin, tmax = 0, 4\n",
        "for i in range(len(raw)):\n",
        "    epochs = mne.Epochs(raw[i], events[i], event_ids[i], tmin, tmax, proj=False, baseline=None, preload=True)\n",
        "\n",
        "    exclude = [\"EOG-left\", \"EOG-central\", \"EOG-right\"]\n",
        "    epochs.drop_channels(exclude)\n",
        "\n",
        "    labels_file = scipy.io.loadmat('./drive/MyDrive/'+data_path[i]+'.mat')\n",
        "\n",
        "    # 打印所有键以便调试\n",
        "    print(f\"MAT file keys for {data_path[i]}: {list(labels_file.keys())}\")\n",
        "\n",
        "    # 新的标签提取方法 - 从data结构体的y字段提取\n",
        "    if 'data' in labels_file:\n",
        "        data_struct = labels_file['data']\n",
        "        print(f\"Data structure shape: {data_struct.shape}\")\n",
        "\n",
        "        # 提取所有trial的标签\n",
        "        all_labels = []\n",
        "        for trial_idx in range(data_struct.shape[1]):\n",
        "            trial_data = data_struct[0, trial_idx]\n",
        "            labels = trial_data['y'][0, 0]  # 提取y字段\n",
        "\n",
        "            if labels.size > 0:  # 只处理有标签的trial\n",
        "                trial_labels = labels.flatten().tolist()\n",
        "                all_labels.extend(trial_labels)\n",
        "                print(f\"Trial {trial_idx+1}: 标签数量 {len(trial_labels)}\")\n",
        "            else:\n",
        "                print(f\"Trial {trial_idx+1}: 无标签\")\n",
        "\n",
        "        labels = np.array(all_labels)\n",
        "        print(f\"提取的总标签数量: {len(labels)}, 唯一标签: {np.unique(labels)}\")\n",
        "\n",
        "        # 显示类别对应关系（用于验证）\n",
        "        if len(all_labels) > 0:\n",
        "            classes = data_struct[0, 0]['classes'][0, 0][0]\n",
        "            print(\"类别对应关系:\")\n",
        "            for class_idx, class_name in enumerate(classes):\n",
        "                print(f\"  标签 {class_idx+1}: {class_name[0]}\")\n",
        "\n",
        "    else:\n",
        "        # 备用方法：使用事件信息生成标签\n",
        "        print(\"使用事件信息生成标签\")\n",
        "        labels = events[i][:, 2]\n",
        "        label_mapping = {7: 1, 8: 2, 9: 3, 10: 4}\n",
        "        if i == 3:  # 特殊处理第4个文件\n",
        "            label_mapping = {5: 1, 6: 2, 7: 3, 8: 4}\n",
        "        labels = np.array([label_mapping.get(event_id, 0) for event_id in labels])\n",
        "        labels = labels[labels != 0]  # 移除无效标签\n",
        "\n",
        "    print(f\"最终标签形状: {labels.shape}, 唯一标签: {np.unique(labels)}\")\n",
        "\n",
        "    epochs_data = epochs.get_data(copy=True)[:, :, :-1]\n",
        "\n",
        "    # 确保标签数量与数据样本数量匹配\n",
        "    n_samples = epochs_data.shape[0]\n",
        "    if len(labels) != n_samples:\n",
        "        print(f\"警告: 标签数量 ({len(labels)}) 与数据样本数量 ({n_samples}) 不匹配\")\n",
        "        # 截取或调整标签以匹配数据数量\n",
        "        min_length = min(len(labels), n_samples)\n",
        "        labels = labels[:min_length]\n",
        "        epochs_data = epochs_data[:min_length]\n",
        "        print(f\"调整后: 标签数量 {len(labels)}, 数据样本数量 {epochs_data.shape[0]}\")\n",
        "\n",
        "    n_channels, n_timepoints = epochs_data.shape[1], epochs_data.shape[2]\n",
        "    epochs_data_flat = epochs_data.reshape(n_samples, -1)\n",
        "\n",
        "    scaler = StandardScaler().fit(epochs_data_flat)\n",
        "    data_scaled = scaler.transform(epochs_data_flat)\n",
        "\n",
        "    data_scaled = data_scaled.reshape(n_samples, n_channels, n_timepoints)\n",
        "\n",
        "    np.savez('2a_train_pre/'+data_path[i]+'.npz', data=data_scaled, label=labels)\n",
        "# 5、创建训练集和验证集数据加载器\n",
        "def create_simple_dataloaders():\n",
        "    # 加载数据\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(1, 10):\n",
        "        train_data = np.load(f'2a_train_pre/A0{i}T.npz')\n",
        "        x_train.append(train_data['data'])\n",
        "        y_train.append(train_data['label'])\n",
        "\n",
        "    # 合并数据\n",
        "    x_train = np.concatenate(x_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "\n",
        "    # 转换为PyTorch张量\n",
        "    x_train = torch.FloatTensor(x_train).unsqueeze(1)\n",
        "    y_train = torch.LongTensor(y_train - 1)\n",
        "\n",
        "    # 创建完整数据集\n",
        "    full_dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "    # 计算训练集和验证集大小\n",
        "    dataset_size = len(full_dataset)\n",
        "    val_size = int(dataset_size * 0.2)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # 划分训练集和验证集\n",
        "    train_data, val_data = random_split(\n",
        "        full_dataset,\n",
        "        [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # 创建DataLoader\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "# 6、训练模型\n",
        "def train_model_process(model, train_loader, val_loader, num_epochs):\n",
        "    # 设定训练所用到的设备，有GPU用GPU没有GPU用CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # 使用Adam优化器，学习率为0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    # 损失函数为交叉熵函数\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # 将模型放入到训练设备中\n",
        "    model = model.to(device)\n",
        "    # 复制当前模型的参数\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # 初始化参数\n",
        "    best_acc = 0.0\n",
        "    train_loss_all = []\n",
        "    val_loss_all = []\n",
        "    train_acc_all = []\n",
        "    val_acc_all = []\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
        "        print(\"-\"*10)\n",
        "\n",
        "        # 初始化参数\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        train_num = 0\n",
        "        val_num = 0\n",
        "\n",
        "        # 对每一个batch训练和计算\n",
        "        for step, (b_x, b_y) in enumerate(train_loader):\n",
        "            # 将特征放入到训练设备中\n",
        "            b_x = b_x.to(device)\n",
        "            # 将标签放入到训练设备中\n",
        "            b_y = b_y.to(device)\n",
        "            # 设置模型为训练模式\n",
        "            model.train()\n",
        "\n",
        "            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n",
        "            output = model(b_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 计算每一个batch的损失函数\n",
        "            loss = criterion(output, b_y)\n",
        "\n",
        "            # 将梯度初始化为0\n",
        "            optimizer.zero_grad()\n",
        "            # 反向传播计算\n",
        "            loss.backward()\n",
        "            # 根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值的作用\n",
        "            optimizer.step()\n",
        "            # 对损失函数进行累加\n",
        "            train_loss += loss.item() * b_x.size(0)\n",
        "            # 如果预测正确，则准确度train_corrects加1\n",
        "            train_corrects += torch.sum(pre_lab == b_y.data)\n",
        "            # 当前用于训练的样本数量\n",
        "            train_num += b_x.size(0)\n",
        "\n",
        "        for step, (b_x, b_y) in enumerate(val_loader):\n",
        "            # 将特征放入到验证设备中\n",
        "            b_x = b_x.to(device)\n",
        "            # 将标签放入到验证设备中\n",
        "            b_y = b_y.to(device)\n",
        "            # 设置模型为评估模式\n",
        "            model.eval()\n",
        "            # 前向传播过程，输入为一个batch，输出为一个batch中对应的预测\n",
        "            output = model(b_x)\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "            # 计算每一个batch的损失函数\n",
        "            loss = criterion(output, b_y)\n",
        "\n",
        "            # 对损失函数进行累加\n",
        "            val_loss += loss.item() * b_x.size(0)\n",
        "            # 如果预测正确，则准确度train_corrects加1\n",
        "            val_corrects += torch.sum(pre_lab == b_y.data)\n",
        "            # 当前用于验证的样本数量\n",
        "            val_num += b_x.size(0)\n",
        "\n",
        "        # 计算并保存每一次迭代的loss值和准确率\n",
        "        # 计算并保存训练集的loss值\n",
        "        train_loss_all.append(train_loss / train_num)\n",
        "        # 计算并保存训练集的准确率\n",
        "        train_acc_all.append(train_corrects.double().item() / train_num)\n",
        "\n",
        "        # 计算并保存验证集的loss值\n",
        "        val_loss_all.append(val_loss / val_num)\n",
        "        # 计算并保存验证集的准确率\n",
        "        val_acc_all.append(val_corrects.double().item() / val_num)\n",
        "\n",
        "        print(\"{} train loss:{:.4f} train acc: {:.4f}\".format(epoch, train_loss_all[-1], train_acc_all[-1]))\n",
        "        print(\"{} val loss:{:.4f} val acc: {:.4f}\".format(epoch, val_loss_all[-1], val_acc_all[-1]))\n",
        "\n",
        "        if val_acc_all[-1] > best_acc:\n",
        "            # 保存当前最高准确度\n",
        "            best_acc = val_acc_all[-1]\n",
        "            # 保存当前最高准确度的模型参数\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # 计算训练和验证的耗时\n",
        "        time_use = time.time() - since\n",
        "        print(\"训练和验证耗费的时间{:.0f}m{:.0f}s\".format(time_use//60, time_use % 60))\n",
        "\n",
        "    # 选择最优参数，保存最优参数的模型\n",
        "    torch.save(best_model_wts, \"best_model.pth\")\n",
        "\n",
        "    train_process = pd.DataFrame(data={\"epoch\": range(num_epochs),\n",
        "\"train_loss_all\": train_loss_all,\n",
        "\"val_loss_all\": val_loss_all,\n",
        "\"train_acc_all\": train_acc_all,\n",
        "\"val_acc_all\": val_acc_all})\n",
        "\n",
        "    return train_process\n",
        "\n",
        "\n",
        "# 7、可视化训练集和验证集的损失函数和准确率\n",
        "def matplot_acc_loss(train_process):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_process['epoch'], train_process.train_loss_all, \"ro-\", label=\"Train loss\")\n",
        "    plt.plot(train_process['epoch'], train_process.val_loss_all, \"bs-\", label=\"Val loss\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_process['epoch'], train_process.train_acc_all, \"ro-\", label=\"Train acc\")\n",
        "    plt.plot(train_process['epoch'], train_process.val_acc_all, \"bs-\", label=\"Val acc\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"acc\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 8、模型开始训练\n",
        "if __name__ == '__main__':\n",
        "    model = EEGNet()\n",
        "    #import pdb; pdb.set_trace()\n",
        "    train_loader, val_loader = create_simple_dataloaders()\n",
        "    train_process = train_model_process(model, train_loader, val_loader, num_epochs=500)\n",
        "    matplot_acc_loss(train_process)\n",
        "\n"
      ],
      "metadata": {
        "id": "aN-b-UvU3Y7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0151551-524c-4344-cc98-576cf47b7140"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A01T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A02T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A03T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A04T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 7)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 标签数量 48\n",
            "Trial 3: 标签数量 48\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A05T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A06T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A07T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A08T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n",
            "MAT file keys for A09T: ['__header__', '__version__', '__globals__', 'data']\n",
            "Data structure shape: (1, 9)\n",
            "Trial 1: 无标签\n",
            "Trial 2: 无标签\n",
            "Trial 3: 无标签\n",
            "Trial 4: 标签数量 48\n",
            "Trial 5: 标签数量 48\n",
            "Trial 6: 标签数量 48\n",
            "Trial 7: 标签数量 48\n",
            "Trial 8: 标签数量 48\n",
            "Trial 9: 标签数量 48\n",
            "提取的总标签数量: 288, 唯一标签: [1 2 3 4]\n",
            "类别对应关系:\n",
            "  标签 1: left hand\n",
            "  标签 2: right hand\n",
            "  标签 3: feet\n",
            "  标签 4: tongue\n",
            "最终标签形状: (288,), 唯一标签: [1 2 3 4]\n",
            "Epoch 0/499\n",
            "----------\n",
            "0 train loss:1.3852 train acc: 0.2512\n",
            "0 val loss:1.3730 val acc: 0.3417\n",
            "训练和验证耗费的时间0m1s\n",
            "Epoch 1/499\n",
            "----------\n",
            "1 train loss:1.3645 train acc: 0.3404\n",
            "1 val loss:1.3563 val acc: 0.3629\n",
            "训练和验证耗费的时间0m2s\n",
            "Epoch 2/499\n",
            "----------\n",
            "2 train loss:1.3429 train acc: 0.3703\n",
            "2 val loss:1.3354 val acc: 0.3958\n",
            "训练和验证耗费的时间0m2s\n",
            "Epoch 3/499\n",
            "----------\n",
            "3 train loss:1.3164 train acc: 0.4176\n",
            "3 val loss:1.3115 val acc: 0.4170\n",
            "训练和验证耗费的时间0m3s\n",
            "Epoch 4/499\n",
            "----------\n",
            "4 train loss:1.2849 train acc: 0.4431\n",
            "4 val loss:1.2855 val acc: 0.4402\n",
            "训练和验证耗费的时间0m4s\n",
            "Epoch 5/499\n",
            "----------\n",
            "5 train loss:1.2605 train acc: 0.4932\n",
            "5 val loss:1.2702 val acc: 0.4556\n",
            "训练和验证耗费的时间0m4s\n",
            "Epoch 6/499\n",
            "----------\n",
            "6 train loss:1.2328 train acc: 0.5058\n",
            "6 val loss:1.2539 val acc: 0.4730\n",
            "训练和验证耗费的时间0m5s\n",
            "Epoch 7/499\n",
            "----------\n",
            "7 train loss:1.2130 train acc: 0.5323\n",
            "7 val loss:1.2440 val acc: 0.4865\n",
            "训练和验证耗费的时间0m5s\n",
            "Epoch 8/499\n",
            "----------\n",
            "8 train loss:1.1993 train acc: 0.5511\n",
            "8 val loss:1.2276 val acc: 0.5077\n",
            "训练和验证耗费的时间0m6s\n",
            "Epoch 9/499\n",
            "----------\n",
            "9 train loss:1.1864 train acc: 0.5627\n",
            "9 val loss:1.2172 val acc: 0.5077\n",
            "训练和验证耗费的时间0m7s\n",
            "Epoch 10/499\n",
            "----------\n",
            "10 train loss:1.1793 train acc: 0.5641\n",
            "10 val loss:1.2151 val acc: 0.5116\n",
            "训练和验证耗费的时间0m7s\n",
            "Epoch 11/499\n",
            "----------\n",
            "11 train loss:1.1746 train acc: 0.5656\n",
            "11 val loss:1.2064 val acc: 0.5154\n",
            "训练和验证耗费的时间0m8s\n",
            "Epoch 12/499\n",
            "----------\n",
            "12 train loss:1.1575 train acc: 0.5897\n",
            "12 val loss:1.1978 val acc: 0.5174\n",
            "训练和验证耗费的时间0m8s\n",
            "Epoch 13/499\n",
            "----------\n",
            "13 train loss:1.1487 train acc: 0.5964\n",
            "13 val loss:1.1894 val acc: 0.5405\n",
            "训练和验证耗费的时间0m9s\n",
            "Epoch 14/499\n",
            "----------\n",
            "14 train loss:1.1468 train acc: 0.5906\n",
            "14 val loss:1.1889 val acc: 0.5444\n",
            "训练和验证耗费的时间0m9s\n",
            "Epoch 15/499\n",
            "----------\n",
            "15 train loss:1.1431 train acc: 0.5988\n",
            "15 val loss:1.1848 val acc: 0.5541\n",
            "训练和验证耗费的时间0m10s\n",
            "Epoch 16/499\n",
            "----------\n",
            "16 train loss:1.1420 train acc: 0.6032\n",
            "16 val loss:1.1825 val acc: 0.5367\n",
            "训练和验证耗费的时间0m11s\n",
            "Epoch 17/499\n",
            "----------\n",
            "17 train loss:1.1329 train acc: 0.6104\n",
            "17 val loss:1.1765 val acc: 0.5483\n",
            "训练和验证耗费的时间0m11s\n",
            "Epoch 18/499\n",
            "----------\n",
            "18 train loss:1.1291 train acc: 0.6138\n",
            "18 val loss:1.1735 val acc: 0.5598\n",
            "训练和验证耗费的时间0m12s\n",
            "Epoch 19/499\n",
            "----------\n",
            "19 train loss:1.1238 train acc: 0.6196\n",
            "19 val loss:1.1799 val acc: 0.5386\n",
            "训练和验证耗费的时间0m13s\n",
            "Epoch 20/499\n",
            "----------\n",
            "20 train loss:1.1215 train acc: 0.6162\n",
            "20 val loss:1.1719 val acc: 0.5541\n",
            "训练和验证耗费的时间0m13s\n",
            "Epoch 21/499\n",
            "----------\n",
            "21 train loss:1.1191 train acc: 0.6201\n",
            "21 val loss:1.1678 val acc: 0.5618\n",
            "训练和验证耗费的时间0m14s\n",
            "Epoch 22/499\n",
            "----------\n",
            "22 train loss:1.1133 train acc: 0.6350\n",
            "22 val loss:1.1610 val acc: 0.5695\n",
            "训练和验证耗费的时间0m14s\n",
            "Epoch 23/499\n",
            "----------\n",
            "23 train loss:1.1062 train acc: 0.6389\n",
            "23 val loss:1.1595 val acc: 0.5734\n",
            "训练和验证耗费的时间0m15s\n",
            "Epoch 24/499\n",
            "----------\n",
            "24 train loss:1.1095 train acc: 0.6307\n",
            "24 val loss:1.1583 val acc: 0.5714\n",
            "训练和验证耗费的时间0m16s\n",
            "Epoch 25/499\n",
            "----------\n",
            "25 train loss:1.1100 train acc: 0.6379\n",
            "25 val loss:1.1536 val acc: 0.5676\n",
            "训练和验证耗费的时间0m16s\n",
            "Epoch 26/499\n",
            "----------\n",
            "26 train loss:1.1014 train acc: 0.6446\n",
            "26 val loss:1.1497 val acc: 0.5830\n",
            "训练和验证耗费的时间0m17s\n",
            "Epoch 27/499\n",
            "----------\n",
            "27 train loss:1.0969 train acc: 0.6490\n",
            "27 val loss:1.1485 val acc: 0.6062\n",
            "训练和验证耗费的时间0m17s\n",
            "Epoch 28/499\n",
            "----------\n",
            "28 train loss:1.0912 train acc: 0.6495\n",
            "28 val loss:1.1522 val acc: 0.5695\n",
            "训练和验证耗费的时间0m18s\n",
            "Epoch 29/499\n",
            "----------\n",
            "29 train loss:1.0966 train acc: 0.6461\n",
            "29 val loss:1.1508 val acc: 0.5811\n",
            "训练和验证耗费的时间0m19s\n",
            "Epoch 30/499\n",
            "----------\n",
            "30 train loss:1.0917 train acc: 0.6528\n",
            "30 val loss:1.1429 val acc: 0.5965\n",
            "训练和验证耗费的时间0m19s\n",
            "Epoch 31/499\n",
            "----------\n",
            "31 train loss:1.0836 train acc: 0.6572\n",
            "31 val loss:1.1383 val acc: 0.6100\n",
            "训练和验证耗费的时间0m20s\n",
            "Epoch 32/499\n",
            "----------\n",
            "32 train loss:1.0882 train acc: 0.6538\n",
            "32 val loss:1.1365 val acc: 0.6120\n",
            "训练和验证耗费的时间0m20s\n",
            "Epoch 33/499\n",
            "----------\n",
            "33 train loss:1.0915 train acc: 0.6548\n",
            "33 val loss:1.1359 val acc: 0.6042\n",
            "训练和验证耗费的时间0m21s\n",
            "Epoch 34/499\n",
            "----------\n",
            "34 train loss:1.0822 train acc: 0.6591\n",
            "34 val loss:1.1324 val acc: 0.5927\n",
            "训练和验证耗费的时间0m22s\n",
            "Epoch 35/499\n",
            "----------\n",
            "35 train loss:1.0864 train acc: 0.6562\n",
            "35 val loss:1.1353 val acc: 0.6023\n",
            "训练和验证耗费的时间0m22s\n",
            "Epoch 36/499\n",
            "----------\n",
            "36 train loss:1.0819 train acc: 0.6620\n",
            "36 val loss:1.1352 val acc: 0.6004\n",
            "训练和验证耗费的时间0m23s\n",
            "Epoch 37/499\n",
            "----------\n",
            "37 train loss:1.0776 train acc: 0.6750\n",
            "37 val loss:1.1321 val acc: 0.6100\n",
            "训练和验证耗费的时间0m24s\n",
            "Epoch 38/499\n",
            "----------\n",
            "38 train loss:1.0780 train acc: 0.6630\n",
            "38 val loss:1.1276 val acc: 0.6023\n",
            "训练和验证耗费的时间0m24s\n",
            "Epoch 39/499\n",
            "----------\n",
            "39 train loss:1.0774 train acc: 0.6615\n",
            "39 val loss:1.1257 val acc: 0.6120\n",
            "训练和验证耗费的时间0m25s\n",
            "Epoch 40/499\n",
            "----------\n",
            "40 train loss:1.0677 train acc: 0.6716\n",
            "40 val loss:1.1300 val acc: 0.6062\n",
            "训练和验证耗费的时间0m25s\n",
            "Epoch 41/499\n",
            "----------\n",
            "41 train loss:1.0788 train acc: 0.6683\n",
            "41 val loss:1.1288 val acc: 0.6100\n",
            "训练和验证耗费的时间0m26s\n",
            "Epoch 42/499\n",
            "----------\n",
            "42 train loss:1.0697 train acc: 0.6702\n",
            "42 val loss:1.1241 val acc: 0.6216\n",
            "训练和验证耗费的时间0m27s\n",
            "Epoch 43/499\n",
            "----------\n",
            "43 train loss:1.0774 train acc: 0.6644\n",
            "43 val loss:1.1279 val acc: 0.6081\n",
            "训练和验证耗费的时间0m27s\n",
            "Epoch 44/499\n",
            "----------\n",
            "44 train loss:1.0677 train acc: 0.6765\n",
            "44 val loss:1.1233 val acc: 0.6120\n",
            "训练和验证耗费的时间0m28s\n",
            "Epoch 45/499\n",
            "----------\n",
            "45 train loss:1.0753 train acc: 0.6731\n",
            "45 val loss:1.1277 val acc: 0.6023\n",
            "训练和验证耗费的时间0m29s\n",
            "Epoch 46/499\n",
            "----------\n",
            "46 train loss:1.0786 train acc: 0.6581\n",
            "46 val loss:1.1213 val acc: 0.6139\n",
            "训练和验证耗费的时间0m29s\n",
            "Epoch 47/499\n",
            "----------\n",
            "47 train loss:1.0687 train acc: 0.6726\n",
            "47 val loss:1.1175 val acc: 0.6178\n",
            "训练和验证耗费的时间0m30s\n",
            "Epoch 48/499\n",
            "----------\n",
            "48 train loss:1.0659 train acc: 0.6745\n",
            "48 val loss:1.1162 val acc: 0.6332\n",
            "训练和验证耗费的时间0m30s\n",
            "Epoch 49/499\n",
            "----------\n",
            "49 train loss:1.0674 train acc: 0.6808\n",
            "49 val loss:1.1146 val acc: 0.6313\n",
            "训练和验证耗费的时间0m31s\n",
            "Epoch 50/499\n",
            "----------\n",
            "50 train loss:1.0591 train acc: 0.6827\n",
            "50 val loss:1.1164 val acc: 0.6255\n",
            "训练和验证耗费的时间0m32s\n",
            "Epoch 51/499\n",
            "----------\n",
            "51 train loss:1.0582 train acc: 0.6861\n",
            "51 val loss:1.1193 val acc: 0.6042\n",
            "训练和验证耗费的时间0m32s\n",
            "Epoch 52/499\n",
            "----------\n",
            "52 train loss:1.0664 train acc: 0.6741\n",
            "52 val loss:1.1192 val acc: 0.6293\n",
            "训练和验证耗费的时间0m33s\n",
            "Epoch 53/499\n",
            "----------\n",
            "53 train loss:1.0572 train acc: 0.6803\n",
            "53 val loss:1.1189 val acc: 0.6081\n",
            "训练和验证耗费的时间0m33s\n",
            "Epoch 54/499\n",
            "----------\n",
            "54 train loss:1.0665 train acc: 0.6765\n",
            "54 val loss:1.1161 val acc: 0.6255\n",
            "训练和验证耗费的时间0m34s\n",
            "Epoch 55/499\n",
            "----------\n",
            "55 train loss:1.0666 train acc: 0.6741\n",
            "55 val loss:1.1155 val acc: 0.6197\n",
            "训练和验证耗费的时间0m35s\n",
            "Epoch 56/499\n",
            "----------\n",
            "56 train loss:1.0639 train acc: 0.6794\n",
            "56 val loss:1.1113 val acc: 0.6216\n",
            "训练和验证耗费的时间0m35s\n",
            "Epoch 57/499\n",
            "----------\n",
            "57 train loss:1.0616 train acc: 0.6808\n",
            "57 val loss:1.1075 val acc: 0.6274\n",
            "训练和验证耗费的时间0m36s\n",
            "Epoch 58/499\n",
            "----------\n",
            "58 train loss:1.0652 train acc: 0.6741\n",
            "58 val loss:1.1057 val acc: 0.6409\n",
            "训练和验证耗费的时间0m36s\n",
            "Epoch 59/499\n",
            "----------\n",
            "59 train loss:1.0487 train acc: 0.6953\n",
            "59 val loss:1.1076 val acc: 0.6274\n",
            "训练和验证耗费的时间0m37s\n",
            "Epoch 60/499\n",
            "----------\n",
            "60 train loss:1.0540 train acc: 0.6861\n",
            "60 val loss:1.1098 val acc: 0.6255\n",
            "训练和验证耗费的时间0m38s\n",
            "Epoch 61/499\n",
            "----------\n",
            "61 train loss:1.0565 train acc: 0.6847\n",
            "61 val loss:1.1044 val acc: 0.6371\n",
            "训练和验证耗费的时间0m38s\n",
            "Epoch 62/499\n",
            "----------\n",
            "62 train loss:1.0511 train acc: 0.6914\n",
            "62 val loss:1.1049 val acc: 0.6255\n",
            "训练和验证耗费的时间0m39s\n",
            "Epoch 63/499\n",
            "----------\n",
            "63 train loss:1.0513 train acc: 0.6938\n",
            "63 val loss:1.1005 val acc: 0.6332\n",
            "训练和验证耗费的时间0m40s\n",
            "Epoch 64/499\n",
            "----------\n",
            "64 train loss:1.0527 train acc: 0.6866\n",
            "64 val loss:1.0984 val acc: 0.6332\n",
            "训练和验证耗费的时间0m40s\n",
            "Epoch 65/499\n",
            "----------\n",
            "65 train loss:1.0640 train acc: 0.6770\n",
            "65 val loss:1.0963 val acc: 0.6371\n",
            "训练和验证耗费的时间0m41s\n",
            "Epoch 66/499\n",
            "----------\n",
            "66 train loss:1.0467 train acc: 0.6948\n",
            "66 val loss:1.0961 val acc: 0.6371\n",
            "训练和验证耗费的时间0m42s\n",
            "Epoch 67/499\n",
            "----------\n",
            "67 train loss:1.0532 train acc: 0.6909\n",
            "67 val loss:1.0970 val acc: 0.6429\n",
            "训练和验证耗费的时间0m42s\n",
            "Epoch 68/499\n",
            "----------\n",
            "68 train loss:1.0584 train acc: 0.6794\n",
            "68 val loss:1.0929 val acc: 0.6371\n",
            "训练和验证耗费的时间0m43s\n",
            "Epoch 69/499\n",
            "----------\n",
            "69 train loss:1.0539 train acc: 0.6847\n",
            "69 val loss:1.0944 val acc: 0.6429\n",
            "训练和验证耗费的时间0m43s\n",
            "Epoch 70/499\n",
            "----------\n",
            "70 train loss:1.0566 train acc: 0.6823\n",
            "70 val loss:1.0924 val acc: 0.6390\n",
            "训练和验证耗费的时间0m44s\n",
            "Epoch 71/499\n",
            "----------\n",
            "71 train loss:1.0461 train acc: 0.6967\n",
            "71 val loss:1.0953 val acc: 0.6467\n",
            "训练和验证耗费的时间0m45s\n",
            "Epoch 72/499\n",
            "----------\n",
            "72 train loss:1.0571 train acc: 0.6837\n",
            "72 val loss:1.0909 val acc: 0.6486\n",
            "训练和验证耗费的时间0m45s\n",
            "Epoch 73/499\n",
            "----------\n",
            "73 train loss:1.0539 train acc: 0.6895\n",
            "73 val loss:1.0886 val acc: 0.6506\n",
            "训练和验证耗费的时间0m46s\n",
            "Epoch 74/499\n",
            "----------\n",
            "74 train loss:1.0459 train acc: 0.6914\n",
            "74 val loss:1.0875 val acc: 0.6506\n",
            "训练和验证耗费的时间0m46s\n",
            "Epoch 75/499\n",
            "----------\n",
            "75 train loss:1.0414 train acc: 0.6982\n",
            "75 val loss:1.0879 val acc: 0.6429\n",
            "训练和验证耗费的时间0m47s\n",
            "Epoch 76/499\n",
            "----------\n",
            "76 train loss:1.0461 train acc: 0.6996\n",
            "76 val loss:1.0884 val acc: 0.6467\n",
            "训练和验证耗费的时间0m48s\n",
            "Epoch 77/499\n",
            "----------\n",
            "77 train loss:1.0336 train acc: 0.7068\n",
            "77 val loss:1.0860 val acc: 0.6680\n",
            "训练和验证耗费的时间0m48s\n",
            "Epoch 78/499\n",
            "----------\n",
            "78 train loss:1.0306 train acc: 0.7117\n",
            "78 val loss:1.0846 val acc: 0.6544\n",
            "训练和验证耗费的时间0m49s\n",
            "Epoch 79/499\n",
            "----------\n",
            "79 train loss:1.0423 train acc: 0.6962\n",
            "79 val loss:1.0799 val acc: 0.6583\n",
            "训练和验证耗费的时间0m49s\n",
            "Epoch 80/499\n",
            "----------\n",
            "80 train loss:1.0393 train acc: 0.6972\n",
            "80 val loss:1.0858 val acc: 0.6583\n",
            "训练和验证耗费的时间0m50s\n",
            "Epoch 81/499\n",
            "----------\n",
            "81 train loss:1.0376 train acc: 0.7015\n",
            "81 val loss:1.0817 val acc: 0.6641\n",
            "训练和验证耗费的时间0m51s\n",
            "Epoch 82/499\n",
            "----------\n",
            "82 train loss:1.0370 train acc: 0.7049\n",
            "82 val loss:1.0819 val acc: 0.6583\n",
            "训练和验证耗费的时间0m51s\n",
            "Epoch 83/499\n",
            "----------\n",
            "83 train loss:1.0428 train acc: 0.6938\n",
            "83 val loss:1.0792 val acc: 0.6622\n",
            "训练和验证耗费的时间0m52s\n",
            "Epoch 84/499\n",
            "----------\n",
            "84 train loss:1.0371 train acc: 0.7078\n",
            "84 val loss:1.0822 val acc: 0.6622\n",
            "训练和验证耗费的时间0m53s\n",
            "Epoch 85/499\n",
            "----------\n",
            "85 train loss:1.0437 train acc: 0.7040\n",
            "85 val loss:1.0831 val acc: 0.6622\n",
            "训练和验证耗费的时间0m53s\n",
            "Epoch 86/499\n",
            "----------\n",
            "86 train loss:1.0364 train acc: 0.7083\n",
            "86 val loss:1.0809 val acc: 0.6525\n",
            "训练和验证耗费的时间0m54s\n",
            "Epoch 87/499\n",
            "----------\n",
            "87 train loss:1.0384 train acc: 0.7049\n",
            "87 val loss:1.0800 val acc: 0.6583\n",
            "训练和验证耗费的时间0m54s\n",
            "Epoch 88/499\n",
            "----------\n",
            "88 train loss:1.0354 train acc: 0.6972\n",
            "88 val loss:1.0761 val acc: 0.6602\n",
            "训练和验证耗费的时间0m55s\n",
            "Epoch 89/499\n",
            "----------\n",
            "89 train loss:1.0297 train acc: 0.7068\n",
            "89 val loss:1.0846 val acc: 0.6467\n",
            "训练和验证耗费的时间0m56s\n",
            "Epoch 90/499\n",
            "----------\n",
            "90 train loss:1.0348 train acc: 0.7001\n",
            "90 val loss:1.0806 val acc: 0.6622\n",
            "训练和验证耗费的时间0m56s\n",
            "Epoch 91/499\n",
            "----------\n",
            "91 train loss:1.0371 train acc: 0.7078\n",
            "91 val loss:1.0748 val acc: 0.6699\n",
            "训练和验证耗费的时间0m57s\n",
            "Epoch 92/499\n",
            "----------\n",
            "92 train loss:1.0358 train acc: 0.7049\n",
            "92 val loss:1.0685 val acc: 0.6834\n",
            "训练和验证耗费的时间0m57s\n",
            "Epoch 93/499\n",
            "----------\n",
            "93 train loss:1.0277 train acc: 0.7194\n",
            "93 val loss:1.0744 val acc: 0.6776\n",
            "训练和验证耗费的时间0m58s\n",
            "Epoch 94/499\n",
            "----------\n",
            "94 train loss:1.0238 train acc: 0.7208\n",
            "94 val loss:1.0710 val acc: 0.6641\n",
            "训练和验证耗费的时间0m59s\n",
            "Epoch 95/499\n",
            "----------\n",
            "95 train loss:1.0342 train acc: 0.7064\n",
            "95 val loss:1.0725 val acc: 0.6680\n",
            "训练和验证耗费的时间1m0s\n",
            "Epoch 96/499\n",
            "----------\n",
            "96 train loss:1.0312 train acc: 0.7131\n",
            "96 val loss:1.0711 val acc: 0.6699\n",
            "训练和验证耗费的时间1m1s\n",
            "Epoch 97/499\n",
            "----------\n",
            "97 train loss:1.0390 train acc: 0.7035\n",
            "97 val loss:1.0714 val acc: 0.6718\n",
            "训练和验证耗费的时间1m1s\n",
            "Epoch 98/499\n",
            "----------\n",
            "98 train loss:1.0342 train acc: 0.7083\n",
            "98 val loss:1.0682 val acc: 0.6680\n",
            "训练和验证耗费的时间1m2s\n",
            "Epoch 99/499\n",
            "----------\n",
            "99 train loss:1.0367 train acc: 0.7035\n",
            "99 val loss:1.0728 val acc: 0.6680\n",
            "训练和验证耗费的时间1m3s\n",
            "Epoch 100/499\n",
            "----------\n",
            "100 train loss:1.0202 train acc: 0.7228\n",
            "100 val loss:1.0685 val acc: 0.6757\n",
            "训练和验证耗费的时间1m3s\n",
            "Epoch 101/499\n",
            "----------\n",
            "101 train loss:1.0370 train acc: 0.7059\n",
            "101 val loss:1.0654 val acc: 0.6815\n",
            "训练和验证耗费的时间1m4s\n",
            "Epoch 102/499\n",
            "----------\n",
            "102 train loss:1.0340 train acc: 0.7088\n",
            "102 val loss:1.0669 val acc: 0.6815\n",
            "训练和验证耗费的时间1m4s\n",
            "Epoch 103/499\n",
            "----------\n",
            "103 train loss:1.0250 train acc: 0.7223\n",
            "103 val loss:1.0712 val acc: 0.6757\n",
            "训练和验证耗费的时间1m5s\n",
            "Epoch 104/499\n",
            "----------\n",
            "104 train loss:1.0194 train acc: 0.7257\n",
            "104 val loss:1.0667 val acc: 0.6660\n",
            "训练和验证耗费的时间1m6s\n",
            "Epoch 105/499\n",
            "----------\n",
            "105 train loss:1.0180 train acc: 0.7266\n",
            "105 val loss:1.0736 val acc: 0.6602\n",
            "训练和验证耗费的时间1m6s\n",
            "Epoch 106/499\n",
            "----------\n",
            "106 train loss:1.0271 train acc: 0.7122\n",
            "106 val loss:1.0699 val acc: 0.6718\n",
            "训练和验证耗费的时间1m7s\n",
            "Epoch 107/499\n",
            "----------\n",
            "107 train loss:1.0301 train acc: 0.7112\n",
            "107 val loss:1.0690 val acc: 0.6737\n",
            "训练和验证耗费的时间1m7s\n",
            "Epoch 108/499\n",
            "----------\n",
            "108 train loss:1.0258 train acc: 0.7213\n",
            "108 val loss:1.0729 val acc: 0.6737\n",
            "训练和验证耗费的时间1m8s\n",
            "Epoch 109/499\n",
            "----------\n",
            "109 train loss:1.0295 train acc: 0.7126\n",
            "109 val loss:1.0747 val acc: 0.6622\n",
            "训练和验证耗费的时间1m9s\n",
            "Epoch 110/499\n",
            "----------\n",
            "110 train loss:1.0211 train acc: 0.7252\n",
            "110 val loss:1.0662 val acc: 0.6680\n",
            "训练和验证耗费的时间1m9s\n",
            "Epoch 111/499\n",
            "----------\n",
            "111 train loss:1.0279 train acc: 0.7131\n",
            "111 val loss:1.0754 val acc: 0.6467\n",
            "训练和验证耗费的时间1m10s\n",
            "Epoch 112/499\n",
            "----------\n",
            "112 train loss:1.0289 train acc: 0.7102\n",
            "112 val loss:1.0726 val acc: 0.6622\n",
            "训练和验证耗费的时间1m10s\n",
            "Epoch 113/499\n",
            "----------\n",
            "113 train loss:1.0170 train acc: 0.7247\n",
            "113 val loss:1.0623 val acc: 0.6815\n",
            "训练和验证耗费的时间1m11s\n",
            "Epoch 114/499\n",
            "----------\n",
            "114 train loss:1.0185 train acc: 0.7266\n",
            "114 val loss:1.0631 val acc: 0.6795\n",
            "训练和验证耗费的时间1m12s\n",
            "Epoch 115/499\n",
            "----------\n",
            "115 train loss:1.0268 train acc: 0.7155\n",
            "115 val loss:1.0659 val acc: 0.6718\n",
            "训练和验证耗费的时间1m12s\n",
            "Epoch 116/499\n",
            "----------\n",
            "116 train loss:1.0183 train acc: 0.7257\n",
            "116 val loss:1.0669 val acc: 0.6757\n",
            "训练和验证耗费的时间1m13s\n",
            "Epoch 117/499\n",
            "----------\n",
            "117 train loss:1.0265 train acc: 0.7160\n",
            "117 val loss:1.0646 val acc: 0.6795\n",
            "训练和验证耗费的时间1m13s\n",
            "Epoch 118/499\n",
            "----------\n",
            "118 train loss:1.0216 train acc: 0.7170\n",
            "118 val loss:1.0644 val acc: 0.6680\n",
            "训练和验证耗费的时间1m14s\n",
            "Epoch 119/499\n",
            "----------\n",
            "119 train loss:1.0178 train acc: 0.7228\n",
            "119 val loss:1.0625 val acc: 0.6776\n",
            "训练和验证耗费的时间1m15s\n",
            "Epoch 120/499\n",
            "----------\n",
            "120 train loss:1.0184 train acc: 0.7218\n",
            "120 val loss:1.0668 val acc: 0.6680\n",
            "训练和验证耗费的时间1m15s\n",
            "Epoch 121/499\n",
            "----------\n",
            "121 train loss:1.0353 train acc: 0.7001\n",
            "121 val loss:1.0613 val acc: 0.6699\n",
            "训练和验证耗费的时间1m16s\n",
            "Epoch 122/499\n",
            "----------\n",
            "122 train loss:1.0260 train acc: 0.7194\n",
            "122 val loss:1.0601 val acc: 0.6795\n",
            "训练和验证耗费的时间1m17s\n",
            "Epoch 123/499\n",
            "----------\n",
            "123 train loss:1.0300 train acc: 0.7093\n",
            "123 val loss:1.0613 val acc: 0.6776\n",
            "训练和验证耗费的时间1m17s\n",
            "Epoch 124/499\n",
            "----------\n",
            "124 train loss:1.0187 train acc: 0.7184\n",
            "124 val loss:1.0625 val acc: 0.6602\n",
            "训练和验证耗费的时间1m18s\n",
            "Epoch 125/499\n",
            "----------\n",
            "125 train loss:1.0304 train acc: 0.7093\n",
            "125 val loss:1.0635 val acc: 0.6815\n",
            "训练和验证耗费的时间1m18s\n",
            "Epoch 126/499\n",
            "----------\n",
            "126 train loss:1.0204 train acc: 0.7208\n",
            "126 val loss:1.0547 val acc: 0.6873\n",
            "训练和验证耗费的时间1m19s\n",
            "Epoch 127/499\n",
            "----------\n",
            "127 train loss:1.0244 train acc: 0.7131\n",
            "127 val loss:1.0584 val acc: 0.6718\n",
            "训练和验证耗费的时间1m20s\n",
            "Epoch 128/499\n",
            "----------\n",
            "128 train loss:1.0227 train acc: 0.7175\n",
            "128 val loss:1.0601 val acc: 0.6660\n",
            "训练和验证耗费的时间1m20s\n",
            "Epoch 129/499\n",
            "----------\n",
            "129 train loss:1.0269 train acc: 0.7131\n",
            "129 val loss:1.0641 val acc: 0.6718\n",
            "训练和验证耗费的时间1m21s\n",
            "Epoch 130/499\n",
            "----------\n",
            "130 train loss:1.0203 train acc: 0.7203\n",
            "130 val loss:1.0618 val acc: 0.6853\n",
            "训练和验证耗费的时间1m22s\n",
            "Epoch 131/499\n",
            "----------\n",
            "131 train loss:1.0206 train acc: 0.7160\n",
            "131 val loss:1.0628 val acc: 0.6795\n",
            "训练和验证耗费的时间1m22s\n",
            "Epoch 132/499\n",
            "----------\n",
            "132 train loss:1.0205 train acc: 0.7194\n",
            "132 val loss:1.0622 val acc: 0.6737\n",
            "训练和验证耗费的时间1m23s\n",
            "Epoch 133/499\n",
            "----------\n",
            "133 train loss:1.0342 train acc: 0.7068\n",
            "133 val loss:1.0652 val acc: 0.6718\n",
            "训练和验证耗费的时间1m23s\n",
            "Epoch 134/499\n",
            "----------\n",
            "134 train loss:1.0294 train acc: 0.7068\n",
            "134 val loss:1.0608 val acc: 0.6815\n",
            "训练和验证耗费的时间1m24s\n",
            "Epoch 135/499\n",
            "----------\n",
            "135 train loss:1.0189 train acc: 0.7218\n",
            "135 val loss:1.0621 val acc: 0.6660\n",
            "训练和验证耗费的时间1m24s\n",
            "Epoch 136/499\n",
            "----------\n",
            "136 train loss:1.0254 train acc: 0.7131\n",
            "136 val loss:1.0573 val acc: 0.6834\n",
            "训练和验证耗费的时间1m25s\n",
            "Epoch 137/499\n",
            "----------\n",
            "137 train loss:1.0201 train acc: 0.7160\n",
            "137 val loss:1.0593 val acc: 0.6795\n",
            "训练和验证耗费的时间1m26s\n",
            "Epoch 138/499\n",
            "----------\n",
            "138 train loss:1.0200 train acc: 0.7232\n",
            "138 val loss:1.0576 val acc: 0.6795\n",
            "训练和验证耗费的时间1m26s\n",
            "Epoch 139/499\n",
            "----------\n",
            "139 train loss:1.0235 train acc: 0.7189\n",
            "139 val loss:1.0592 val acc: 0.6680\n",
            "训练和验证耗费的时间1m27s\n",
            "Epoch 140/499\n",
            "----------\n",
            "140 train loss:1.0207 train acc: 0.7155\n",
            "140 val loss:1.0555 val acc: 0.6873\n",
            "训练和验证耗费的时间1m27s\n",
            "Epoch 141/499\n",
            "----------\n",
            "141 train loss:1.0184 train acc: 0.7175\n",
            "141 val loss:1.0505 val acc: 0.6892\n",
            "训练和验证耗费的时间1m28s\n",
            "Epoch 142/499\n",
            "----------\n",
            "142 train loss:1.0222 train acc: 0.7179\n",
            "142 val loss:1.0575 val acc: 0.6699\n",
            "训练和验证耗费的时间1m29s\n",
            "Epoch 143/499\n",
            "----------\n",
            "143 train loss:1.0164 train acc: 0.7266\n",
            "143 val loss:1.0555 val acc: 0.6853\n",
            "训练和验证耗费的时间1m29s\n",
            "Epoch 144/499\n",
            "----------\n",
            "144 train loss:1.0214 train acc: 0.7203\n",
            "144 val loss:1.0488 val acc: 0.6950\n",
            "训练和验证耗费的时间1m30s\n",
            "Epoch 145/499\n",
            "----------\n",
            "145 train loss:1.0200 train acc: 0.7252\n",
            "145 val loss:1.0564 val acc: 0.6853\n",
            "训练和验证耗费的时间1m31s\n",
            "Epoch 146/499\n",
            "----------\n",
            "146 train loss:1.0064 train acc: 0.7358\n",
            "146 val loss:1.0558 val acc: 0.6892\n",
            "训练和验证耗费的时间1m31s\n",
            "Epoch 147/499\n",
            "----------\n",
            "147 train loss:1.0116 train acc: 0.7319\n",
            "147 val loss:1.0541 val acc: 0.6795\n",
            "训练和验证耗费的时间1m32s\n",
            "Epoch 148/499\n",
            "----------\n",
            "148 train loss:1.0140 train acc: 0.7271\n",
            "148 val loss:1.0504 val acc: 0.6892\n",
            "训练和验证耗费的时间1m32s\n",
            "Epoch 149/499\n",
            "----------\n",
            "149 train loss:1.0208 train acc: 0.7150\n",
            "149 val loss:1.0530 val acc: 0.6834\n",
            "训练和验证耗费的时间1m33s\n",
            "Epoch 150/499\n",
            "----------\n",
            "150 train loss:1.0190 train acc: 0.7179\n",
            "150 val loss:1.0497 val acc: 0.6776\n",
            "训练和验证耗费的时间1m34s\n",
            "Epoch 151/499\n",
            "----------\n",
            "151 train loss:1.0130 train acc: 0.7266\n",
            "151 val loss:1.0424 val acc: 0.6988\n",
            "训练和验证耗费的时间1m34s\n",
            "Epoch 152/499\n",
            "----------\n",
            "152 train loss:1.0184 train acc: 0.7223\n",
            "152 val loss:1.0441 val acc: 0.6950\n",
            "训练和验证耗费的时间1m35s\n",
            "Epoch 153/499\n",
            "----------\n",
            "153 train loss:1.0138 train acc: 0.7285\n",
            "153 val loss:1.0511 val acc: 0.6795\n",
            "训练和验证耗费的时间1m35s\n",
            "Epoch 154/499\n",
            "----------\n",
            "154 train loss:1.0222 train acc: 0.7194\n",
            "154 val loss:1.0426 val acc: 0.6969\n",
            "训练和验证耗费的时间1m36s\n",
            "Epoch 155/499\n",
            "----------\n",
            "155 train loss:1.0041 train acc: 0.7363\n",
            "155 val loss:1.0460 val acc: 0.6853\n",
            "训练和验证耗费的时间1m37s\n",
            "Epoch 156/499\n",
            "----------\n",
            "156 train loss:1.0109 train acc: 0.7290\n",
            "156 val loss:1.0480 val acc: 0.6873\n",
            "训练和验证耗费的时间1m37s\n",
            "Epoch 157/499\n",
            "----------\n",
            "157 train loss:1.0195 train acc: 0.7194\n",
            "157 val loss:1.0597 val acc: 0.6641\n",
            "训练和验证耗费的时间1m38s\n",
            "Epoch 158/499\n",
            "----------\n",
            "158 train loss:1.0262 train acc: 0.7184\n",
            "158 val loss:1.0476 val acc: 0.6892\n",
            "训练和验证耗费的时间1m38s\n",
            "Epoch 159/499\n",
            "----------\n",
            "159 train loss:1.0156 train acc: 0.7213\n",
            "159 val loss:1.0447 val acc: 0.6950\n",
            "训练和验证耗费的时间1m39s\n",
            "Epoch 160/499\n",
            "----------\n",
            "160 train loss:1.0078 train acc: 0.7281\n",
            "160 val loss:1.0438 val acc: 0.6969\n",
            "训练和验证耗费的时间1m40s\n",
            "Epoch 161/499\n",
            "----------\n",
            "161 train loss:1.0095 train acc: 0.7295\n",
            "161 val loss:1.0489 val acc: 0.6892\n",
            "训练和验证耗费的时间1m40s\n",
            "Epoch 162/499\n",
            "----------\n",
            "162 train loss:1.0181 train acc: 0.7194\n",
            "162 val loss:1.0426 val acc: 0.6969\n",
            "训练和验证耗费的时间1m41s\n",
            "Epoch 163/499\n",
            "----------\n",
            "163 train loss:1.0080 train acc: 0.7338\n",
            "163 val loss:1.0516 val acc: 0.6853\n",
            "训练和验证耗费的时间1m41s\n",
            "Epoch 164/499\n",
            "----------\n",
            "164 train loss:1.0177 train acc: 0.7237\n",
            "164 val loss:1.0451 val acc: 0.6911\n",
            "训练和验证耗费的时间1m42s\n",
            "Epoch 165/499\n",
            "----------\n",
            "165 train loss:1.0125 train acc: 0.7314\n",
            "165 val loss:1.0540 val acc: 0.6718\n",
            "训练和验证耗费的时间1m43s\n",
            "Epoch 166/499\n",
            "----------\n",
            "166 train loss:1.0113 train acc: 0.7276\n",
            "166 val loss:1.0493 val acc: 0.6815\n",
            "训练和验证耗费的时间1m43s\n",
            "Epoch 167/499\n",
            "----------\n",
            "167 train loss:1.0139 train acc: 0.7247\n",
            "167 val loss:1.0465 val acc: 0.6911\n",
            "训练和验证耗费的时间1m44s\n",
            "Epoch 168/499\n",
            "----------\n",
            "168 train loss:1.0182 train acc: 0.7199\n",
            "168 val loss:1.0573 val acc: 0.6737\n",
            "训练和验证耗费的时间1m45s\n",
            "Epoch 169/499\n",
            "----------\n",
            "169 train loss:1.0153 train acc: 0.7257\n",
            "169 val loss:1.0525 val acc: 0.6815\n",
            "训练和验证耗费的时间1m45s\n",
            "Epoch 170/499\n",
            "----------\n",
            "170 train loss:1.0068 train acc: 0.7353\n",
            "170 val loss:1.0480 val acc: 0.6834\n",
            "训练和验证耗费的时间1m46s\n",
            "Epoch 171/499\n",
            "----------\n",
            "171 train loss:1.0120 train acc: 0.7276\n",
            "171 val loss:1.0514 val acc: 0.6815\n",
            "训练和验证耗费的时间1m46s\n",
            "Epoch 172/499\n",
            "----------\n",
            "172 train loss:1.0200 train acc: 0.7175\n",
            "172 val loss:1.0470 val acc: 0.6892\n",
            "训练和验证耗费的时间1m47s\n",
            "Epoch 173/499\n",
            "----------\n",
            "173 train loss:1.0236 train acc: 0.7112\n",
            "173 val loss:1.0477 val acc: 0.6911\n",
            "训练和验证耗费的时间1m48s\n",
            "Epoch 174/499\n",
            "----------\n",
            "174 train loss:1.0127 train acc: 0.7228\n",
            "174 val loss:1.0531 val acc: 0.6795\n",
            "训练和验证耗费的时间1m48s\n",
            "Epoch 175/499\n",
            "----------\n",
            "175 train loss:1.0134 train acc: 0.7232\n",
            "175 val loss:1.0445 val acc: 0.6911\n",
            "训练和验证耗费的时间1m49s\n",
            "Epoch 176/499\n",
            "----------\n",
            "176 train loss:1.0143 train acc: 0.7194\n",
            "176 val loss:1.0548 val acc: 0.6757\n",
            "训练和验证耗费的时间1m49s\n",
            "Epoch 177/499\n",
            "----------\n",
            "177 train loss:1.0017 train acc: 0.7449\n",
            "177 val loss:1.0607 val acc: 0.6680\n",
            "训练和验证耗费的时间1m50s\n",
            "Epoch 178/499\n",
            "----------\n",
            "178 train loss:1.0114 train acc: 0.7305\n",
            "178 val loss:1.0538 val acc: 0.6737\n",
            "训练和验证耗费的时间1m51s\n",
            "Epoch 179/499\n",
            "----------\n",
            "179 train loss:1.0109 train acc: 0.7329\n",
            "179 val loss:1.0512 val acc: 0.6680\n",
            "训练和验证耗费的时间1m51s\n",
            "Epoch 180/499\n",
            "----------\n",
            "180 train loss:1.0131 train acc: 0.7261\n",
            "180 val loss:1.0435 val acc: 0.6892\n",
            "训练和验证耗费的时间1m52s\n",
            "Epoch 181/499\n",
            "----------\n",
            "181 train loss:1.0017 train acc: 0.7454\n",
            "181 val loss:1.0460 val acc: 0.6931\n",
            "训练和验证耗费的时间1m52s\n",
            "Epoch 182/499\n",
            "----------\n",
            "182 train loss:1.0127 train acc: 0.7261\n",
            "182 val loss:1.0504 val acc: 0.6795\n",
            "训练和验证耗费的时间1m53s\n",
            "Epoch 183/499\n",
            "----------\n",
            "183 train loss:1.0049 train acc: 0.7411\n",
            "183 val loss:1.0447 val acc: 0.6853\n",
            "训练和验证耗费的时间1m54s\n",
            "Epoch 184/499\n",
            "----------\n",
            "184 train loss:1.0041 train acc: 0.7353\n",
            "184 val loss:1.0447 val acc: 0.6911\n",
            "训练和验证耗费的时间1m54s\n",
            "Epoch 185/499\n",
            "----------\n",
            "185 train loss:1.0099 train acc: 0.7271\n",
            "185 val loss:1.0516 val acc: 0.6776\n",
            "训练和验证耗费的时间1m55s\n",
            "Epoch 186/499\n",
            "----------\n",
            "186 train loss:1.0082 train acc: 0.7305\n",
            "186 val loss:1.0310 val acc: 0.7046\n",
            "训练和验证耗费的时间1m56s\n",
            "Epoch 187/499\n",
            "----------\n",
            "187 train loss:1.0160 train acc: 0.7242\n",
            "187 val loss:1.0376 val acc: 0.6969\n",
            "训练和验证耗费的时间1m56s\n",
            "Epoch 188/499\n",
            "----------\n",
            "188 train loss:1.0035 train acc: 0.7392\n",
            "188 val loss:1.0368 val acc: 0.7008\n",
            "训练和验证耗费的时间1m57s\n",
            "Epoch 189/499\n",
            "----------\n",
            "189 train loss:1.0102 train acc: 0.7281\n",
            "189 val loss:1.0358 val acc: 0.7085\n",
            "训练和验证耗费的时间1m57s\n",
            "Epoch 190/499\n",
            "----------\n",
            "190 train loss:1.0190 train acc: 0.7223\n",
            "190 val loss:1.0305 val acc: 0.7124\n",
            "训练和验证耗费的时间1m58s\n",
            "Epoch 191/499\n",
            "----------\n",
            "191 train loss:1.0164 train acc: 0.7189\n",
            "191 val loss:1.0309 val acc: 0.7143\n",
            "训练和验证耗费的时间1m59s\n",
            "Epoch 192/499\n",
            "----------\n",
            "192 train loss:1.0030 train acc: 0.7392\n",
            "192 val loss:1.0344 val acc: 0.7143\n",
            "训练和验证耗费的时间1m59s\n",
            "Epoch 193/499\n",
            "----------\n",
            "193 train loss:1.0056 train acc: 0.7329\n",
            "193 val loss:1.0331 val acc: 0.7027\n",
            "训练和验证耗费的时间1m60s\n",
            "Epoch 194/499\n",
            "----------\n",
            "194 train loss:1.0014 train acc: 0.7420\n",
            "194 val loss:1.0449 val acc: 0.6931\n",
            "训练和验证耗费的时间2m1s\n",
            "Epoch 195/499\n",
            "----------\n",
            "195 train loss:1.0074 train acc: 0.7324\n",
            "195 val loss:1.0354 val acc: 0.7066\n",
            "训练和验证耗费的时间2m1s\n",
            "Epoch 196/499\n",
            "----------\n",
            "196 train loss:1.0039 train acc: 0.7377\n",
            "196 val loss:1.0427 val acc: 0.6950\n",
            "训练和验证耗费的时间2m2s\n",
            "Epoch 197/499\n",
            "----------\n",
            "197 train loss:1.0032 train acc: 0.7387\n",
            "197 val loss:1.0413 val acc: 0.6911\n",
            "训练和验证耗费的时间2m2s\n",
            "Epoch 198/499\n",
            "----------\n",
            "198 train loss:0.9927 train acc: 0.7531\n",
            "198 val loss:1.0335 val acc: 0.7104\n",
            "训练和验证耗费的时间2m3s\n",
            "Epoch 199/499\n",
            "----------\n",
            "199 train loss:0.9994 train acc: 0.7382\n",
            "199 val loss:1.0351 val acc: 0.7143\n",
            "训练和验证耗费的时间2m4s\n",
            "Epoch 200/499\n",
            "----------\n",
            "200 train loss:1.0067 train acc: 0.7310\n",
            "200 val loss:1.0362 val acc: 0.7008\n",
            "训练和验证耗费的时间2m4s\n",
            "Epoch 201/499\n",
            "----------\n",
            "201 train loss:1.0056 train acc: 0.7353\n",
            "201 val loss:1.0366 val acc: 0.6988\n",
            "训练和验证耗费的时间2m5s\n",
            "Epoch 202/499\n",
            "----------\n",
            "202 train loss:1.0079 train acc: 0.7382\n",
            "202 val loss:1.0357 val acc: 0.7008\n",
            "训练和验证耗费的时间2m5s\n",
            "Epoch 203/499\n",
            "----------\n",
            "203 train loss:1.0103 train acc: 0.7338\n",
            "203 val loss:1.0468 val acc: 0.6931\n",
            "训练和验证耗费的时间2m6s\n",
            "Epoch 204/499\n",
            "----------\n",
            "204 train loss:1.0060 train acc: 0.7382\n",
            "204 val loss:1.0472 val acc: 0.6892\n",
            "训练和验证耗费的时间2m7s\n",
            "Epoch 205/499\n",
            "----------\n",
            "205 train loss:0.9989 train acc: 0.7411\n",
            "205 val loss:1.0420 val acc: 0.6988\n",
            "训练和验证耗费的时间2m7s\n",
            "Epoch 206/499\n",
            "----------\n",
            "206 train loss:0.9888 train acc: 0.7517\n",
            "206 val loss:1.0359 val acc: 0.7066\n",
            "训练和验证耗费的时间2m8s\n",
            "Epoch 207/499\n",
            "----------\n",
            "207 train loss:1.0044 train acc: 0.7392\n",
            "207 val loss:1.0519 val acc: 0.6834\n",
            "训练和验证耗费的时间2m8s\n",
            "Epoch 208/499\n",
            "----------\n",
            "208 train loss:1.0001 train acc: 0.7425\n",
            "208 val loss:1.0391 val acc: 0.6969\n",
            "训练和验证耗费的时间2m9s\n",
            "Epoch 209/499\n",
            "----------\n",
            "209 train loss:1.0032 train acc: 0.7387\n",
            "209 val loss:1.0327 val acc: 0.7027\n",
            "训练和验证耗费的时间2m10s\n",
            "Epoch 210/499\n",
            "----------\n",
            "210 train loss:1.0128 train acc: 0.7276\n",
            "210 val loss:1.0400 val acc: 0.6911\n",
            "训练和验证耗费的时间2m10s\n",
            "Epoch 211/499\n",
            "----------\n",
            "211 train loss:0.9984 train acc: 0.7411\n",
            "211 val loss:1.0428 val acc: 0.6931\n",
            "训练和验证耗费的时间2m11s\n",
            "Epoch 212/499\n",
            "----------\n",
            "212 train loss:1.0077 train acc: 0.7290\n",
            "212 val loss:1.0381 val acc: 0.6988\n",
            "训练和验证耗费的时间2m12s\n",
            "Epoch 213/499\n",
            "----------\n",
            "213 train loss:1.0090 train acc: 0.7300\n",
            "213 val loss:1.0380 val acc: 0.7104\n",
            "训练和验证耗费的时间2m12s\n",
            "Epoch 214/499\n",
            "----------\n",
            "214 train loss:0.9926 train acc: 0.7498\n",
            "214 val loss:1.0345 val acc: 0.7027\n",
            "训练和验证耗费的时间2m13s\n",
            "Epoch 215/499\n",
            "----------\n",
            "215 train loss:1.0078 train acc: 0.7310\n",
            "215 val loss:1.0347 val acc: 0.7027\n",
            "训练和验证耗费的时间2m13s\n",
            "Epoch 216/499\n",
            "----------\n",
            "216 train loss:1.0017 train acc: 0.7329\n",
            "216 val loss:1.0384 val acc: 0.6950\n",
            "训练和验证耗费的时间2m14s\n",
            "Epoch 217/499\n",
            "----------\n",
            "217 train loss:1.0001 train acc: 0.7464\n",
            "217 val loss:1.0299 val acc: 0.7085\n",
            "训练和验证耗费的时间2m15s\n",
            "Epoch 218/499\n",
            "----------\n",
            "218 train loss:1.0030 train acc: 0.7295\n",
            "218 val loss:1.0283 val acc: 0.7239\n",
            "训练和验证耗费的时间2m15s\n",
            "Epoch 219/499\n",
            "----------\n",
            "219 train loss:0.9944 train acc: 0.7435\n",
            "219 val loss:1.0267 val acc: 0.7143\n",
            "训练和验证耗费的时间2m16s\n",
            "Epoch 220/499\n",
            "----------\n",
            "220 train loss:1.0081 train acc: 0.7314\n",
            "220 val loss:1.0230 val acc: 0.7181\n",
            "训练和验证耗费的时间2m16s\n",
            "Epoch 221/499\n",
            "----------\n",
            "221 train loss:1.0035 train acc: 0.7411\n",
            "221 val loss:1.0323 val acc: 0.7124\n",
            "训练和验证耗费的时间2m17s\n",
            "Epoch 222/499\n",
            "----------\n",
            "222 train loss:0.9936 train acc: 0.7435\n",
            "222 val loss:1.0369 val acc: 0.7008\n",
            "训练和验证耗费的时间2m18s\n",
            "Epoch 223/499\n",
            "----------\n",
            "223 train loss:0.9956 train acc: 0.7445\n",
            "223 val loss:1.0227 val acc: 0.7220\n",
            "训练和验证耗费的时间2m18s\n",
            "Epoch 224/499\n",
            "----------\n",
            "224 train loss:0.9937 train acc: 0.7546\n",
            "224 val loss:1.0326 val acc: 0.7085\n",
            "训练和验证耗费的时间2m19s\n",
            "Epoch 225/499\n",
            "----------\n",
            "225 train loss:1.0081 train acc: 0.7285\n",
            "225 val loss:1.0343 val acc: 0.7008\n",
            "训练和验证耗费的时间2m19s\n",
            "Epoch 226/499\n",
            "----------\n",
            "226 train loss:1.0045 train acc: 0.7348\n",
            "226 val loss:1.0309 val acc: 0.7124\n",
            "训练和验证耗费的时间2m20s\n",
            "Epoch 227/499\n",
            "----------\n",
            "227 train loss:0.9991 train acc: 0.7396\n",
            "227 val loss:1.0373 val acc: 0.7124\n",
            "训练和验证耗费的时间2m21s\n",
            "Epoch 228/499\n",
            "----------\n",
            "228 train loss:1.0053 train acc: 0.7310\n",
            "228 val loss:1.0414 val acc: 0.6873\n",
            "训练和验证耗费的时间2m21s\n",
            "Epoch 229/499\n",
            "----------\n",
            "229 train loss:1.0037 train acc: 0.7353\n",
            "229 val loss:1.0395 val acc: 0.6950\n",
            "训练和验证耗费的时间2m22s\n",
            "Epoch 230/499\n",
            "----------\n",
            "230 train loss:0.9977 train acc: 0.7440\n",
            "230 val loss:1.0379 val acc: 0.7046\n",
            "训练和验证耗费的时间2m23s\n",
            "Epoch 231/499\n",
            "----------\n",
            "231 train loss:0.9914 train acc: 0.7522\n",
            "231 val loss:1.0299 val acc: 0.7143\n",
            "训练和验证耗费的时间2m23s\n",
            "Epoch 232/499\n",
            "----------\n",
            "232 train loss:1.0021 train acc: 0.7377\n",
            "232 val loss:1.0343 val acc: 0.6950\n",
            "训练和验证耗费的时间2m24s\n",
            "Epoch 233/499\n",
            "----------\n",
            "233 train loss:0.9938 train acc: 0.7459\n",
            "233 val loss:1.0394 val acc: 0.7027\n",
            "训练和验证耗费的时间2m24s\n",
            "Epoch 234/499\n",
            "----------\n",
            "234 train loss:1.0084 train acc: 0.7338\n",
            "234 val loss:1.0387 val acc: 0.7027\n",
            "训练和验证耗费的时间2m25s\n",
            "Epoch 235/499\n",
            "----------\n",
            "235 train loss:1.0057 train acc: 0.7343\n",
            "235 val loss:1.0352 val acc: 0.6911\n",
            "训练和验证耗费的时间2m26s\n",
            "Epoch 236/499\n",
            "----------\n",
            "236 train loss:1.0099 train acc: 0.7314\n",
            "236 val loss:1.0333 val acc: 0.7124\n",
            "训练和验证耗费的时间2m26s\n",
            "Epoch 237/499\n",
            "----------\n",
            "237 train loss:0.9855 train acc: 0.7575\n",
            "237 val loss:1.0246 val acc: 0.7143\n",
            "训练和验证耗费的时间2m27s\n",
            "Epoch 238/499\n",
            "----------\n",
            "238 train loss:1.0056 train acc: 0.7338\n",
            "238 val loss:1.0310 val acc: 0.7124\n",
            "训练和验证耗费的时间2m27s\n",
            "Epoch 239/499\n",
            "----------\n",
            "239 train loss:0.9933 train acc: 0.7469\n",
            "239 val loss:1.0289 val acc: 0.7066\n",
            "训练和验证耗费的时间2m28s\n",
            "Epoch 240/499\n",
            "----------\n",
            "240 train loss:0.9966 train acc: 0.7416\n",
            "240 val loss:1.0275 val acc: 0.7104\n",
            "训练和验证耗费的时间2m29s\n",
            "Epoch 241/499\n",
            "----------\n",
            "241 train loss:1.0005 train acc: 0.7396\n",
            "241 val loss:1.0321 val acc: 0.7066\n",
            "训练和验证耗费的时间2m29s\n",
            "Epoch 242/499\n",
            "----------\n",
            "242 train loss:0.9918 train acc: 0.7493\n",
            "242 val loss:1.0260 val acc: 0.7124\n",
            "训练和验证耗费的时间2m30s\n",
            "Epoch 243/499\n",
            "----------\n",
            "243 train loss:1.0011 train acc: 0.7401\n",
            "243 val loss:1.0303 val acc: 0.7046\n",
            "训练和验证耗费的时间2m30s\n",
            "Epoch 244/499\n",
            "----------\n",
            "244 train loss:0.9998 train acc: 0.7387\n",
            "244 val loss:1.0290 val acc: 0.7066\n",
            "训练和验证耗费的时间2m31s\n",
            "Epoch 245/499\n",
            "----------\n",
            "245 train loss:1.0023 train acc: 0.7329\n",
            "245 val loss:1.0285 val acc: 0.7085\n",
            "训练和验证耗费的时间2m32s\n",
            "Epoch 246/499\n",
            "----------\n",
            "246 train loss:0.9908 train acc: 0.7531\n",
            "246 val loss:1.0337 val acc: 0.6969\n",
            "训练和验证耗费的时间2m32s\n",
            "Epoch 247/499\n",
            "----------\n",
            "247 train loss:0.9992 train acc: 0.7387\n",
            "247 val loss:1.0327 val acc: 0.7085\n",
            "训练和验证耗费的时间2m33s\n",
            "Epoch 248/499\n",
            "----------\n",
            "248 train loss:0.9922 train acc: 0.7454\n",
            "248 val loss:1.0316 val acc: 0.7008\n",
            "训练和验证耗费的时间2m33s\n",
            "Epoch 249/499\n",
            "----------\n",
            "249 train loss:0.9999 train acc: 0.7387\n",
            "249 val loss:1.0304 val acc: 0.7085\n",
            "训练和验证耗费的时间2m34s\n",
            "Epoch 250/499\n",
            "----------\n",
            "250 train loss:0.9997 train acc: 0.7435\n",
            "250 val loss:1.0269 val acc: 0.7143\n",
            "训练和验证耗费的时间2m35s\n",
            "Epoch 251/499\n",
            "----------\n",
            "251 train loss:0.9912 train acc: 0.7454\n",
            "251 val loss:1.0333 val acc: 0.6988\n",
            "训练和验证耗费的时间2m35s\n",
            "Epoch 252/499\n",
            "----------\n",
            "252 train loss:0.9974 train acc: 0.7420\n",
            "252 val loss:1.0385 val acc: 0.6931\n",
            "训练和验证耗费的时间2m36s\n",
            "Epoch 253/499\n",
            "----------\n",
            "253 train loss:0.9994 train acc: 0.7377\n",
            "253 val loss:1.0389 val acc: 0.6815\n",
            "训练和验证耗费的时间2m37s\n",
            "Epoch 254/499\n",
            "----------\n",
            "254 train loss:0.9961 train acc: 0.7411\n",
            "254 val loss:1.0324 val acc: 0.6969\n",
            "训练和验证耗费的时间2m37s\n",
            "Epoch 255/499\n",
            "----------\n",
            "255 train loss:0.9923 train acc: 0.7469\n",
            "255 val loss:1.0299 val acc: 0.7104\n",
            "训练和验证耗费的时间2m38s\n",
            "Epoch 256/499\n",
            "----------\n",
            "256 train loss:0.9913 train acc: 0.7473\n",
            "256 val loss:1.0326 val acc: 0.7066\n",
            "训练和验证耗费的时间2m39s\n",
            "Epoch 257/499\n",
            "----------\n",
            "257 train loss:0.9981 train acc: 0.7435\n",
            "257 val loss:1.0376 val acc: 0.7008\n",
            "训练和验证耗费的时间2m39s\n",
            "Epoch 258/499\n",
            "----------\n",
            "258 train loss:0.9943 train acc: 0.7502\n",
            "258 val loss:1.0356 val acc: 0.6950\n",
            "训练和验证耗费的时间2m40s\n",
            "Epoch 259/499\n",
            "----------\n",
            "259 train loss:1.0034 train acc: 0.7353\n",
            "259 val loss:1.0384 val acc: 0.7066\n",
            "训练和验证耗费的时间2m40s\n",
            "Epoch 260/499\n",
            "----------\n",
            "260 train loss:0.9941 train acc: 0.7488\n",
            "260 val loss:1.0302 val acc: 0.7124\n",
            "训练和验证耗费的时间2m41s\n",
            "Epoch 261/499\n",
            "----------\n",
            "261 train loss:0.9796 train acc: 0.7642\n",
            "261 val loss:1.0286 val acc: 0.7201\n",
            "训练和验证耗费的时间2m42s\n",
            "Epoch 262/499\n",
            "----------\n",
            "262 train loss:0.9970 train acc: 0.7420\n",
            "262 val loss:1.0362 val acc: 0.7008\n",
            "训练和验证耗费的时间2m42s\n",
            "Epoch 263/499\n",
            "----------\n",
            "263 train loss:0.9928 train acc: 0.7459\n",
            "263 val loss:1.0400 val acc: 0.6969\n",
            "训练和验证耗费的时间2m43s\n",
            "Epoch 264/499\n",
            "----------\n",
            "264 train loss:0.9873 train acc: 0.7541\n",
            "264 val loss:1.0361 val acc: 0.7124\n",
            "训练和验证耗费的时间2m43s\n",
            "Epoch 265/499\n",
            "----------\n",
            "265 train loss:0.9914 train acc: 0.7507\n",
            "265 val loss:1.0253 val acc: 0.7181\n",
            "训练和验证耗费的时间2m44s\n",
            "Epoch 266/499\n",
            "----------\n",
            "266 train loss:0.9978 train acc: 0.7416\n",
            "266 val loss:1.0363 val acc: 0.6931\n",
            "训练和验证耗费的时间2m45s\n",
            "Epoch 267/499\n",
            "----------\n",
            "267 train loss:0.9938 train acc: 0.7478\n",
            "267 val loss:1.0392 val acc: 0.6988\n",
            "训练和验证耗费的时间2m45s\n",
            "Epoch 268/499\n",
            "----------\n",
            "268 train loss:1.0026 train acc: 0.7358\n",
            "268 val loss:1.0388 val acc: 0.7046\n",
            "训练和验证耗费的时间2m46s\n",
            "Epoch 269/499\n",
            "----------\n",
            "269 train loss:0.9883 train acc: 0.7527\n",
            "269 val loss:1.0307 val acc: 0.7085\n",
            "训练和验证耗费的时间2m46s\n",
            "Epoch 270/499\n",
            "----------\n",
            "270 train loss:0.9979 train acc: 0.7406\n",
            "270 val loss:1.0224 val acc: 0.7143\n",
            "训练和验证耗费的时间2m47s\n",
            "Epoch 271/499\n",
            "----------\n",
            "271 train loss:0.9978 train acc: 0.7440\n",
            "271 val loss:1.0291 val acc: 0.7027\n",
            "训练和验证耗费的时间2m48s\n",
            "Epoch 272/499\n",
            "----------\n",
            "272 train loss:0.9918 train acc: 0.7498\n",
            "272 val loss:1.0264 val acc: 0.7066\n",
            "训练和验证耗费的时间2m48s\n",
            "Epoch 273/499\n",
            "----------\n",
            "273 train loss:0.9934 train acc: 0.7483\n",
            "273 val loss:1.0266 val acc: 0.7124\n",
            "训练和验证耗费的时间2m49s\n",
            "Epoch 274/499\n",
            "----------\n",
            "274 train loss:0.9882 train acc: 0.7527\n",
            "274 val loss:1.0263 val acc: 0.7162\n",
            "训练和验证耗费的时间2m49s\n",
            "Epoch 275/499\n",
            "----------\n",
            "275 train loss:0.9863 train acc: 0.7517\n",
            "275 val loss:1.0249 val acc: 0.7104\n",
            "训练和验证耗费的时间2m50s\n",
            "Epoch 276/499\n",
            "----------\n",
            "276 train loss:0.9971 train acc: 0.7454\n",
            "276 val loss:1.0265 val acc: 0.7201\n",
            "训练和验证耗费的时间2m51s\n",
            "Epoch 277/499\n",
            "----------\n",
            "277 train loss:0.9884 train acc: 0.7536\n",
            "277 val loss:1.0316 val acc: 0.7066\n",
            "训练和验证耗费的时间2m51s\n",
            "Epoch 278/499\n",
            "----------\n",
            "278 train loss:0.9838 train acc: 0.7604\n",
            "278 val loss:1.0257 val acc: 0.7143\n",
            "训练和验证耗费的时间2m52s\n",
            "Epoch 279/499\n",
            "----------\n",
            "279 train loss:0.9902 train acc: 0.7502\n",
            "279 val loss:1.0331 val acc: 0.7104\n",
            "训练和验证耗费的时间2m53s\n",
            "Epoch 280/499\n",
            "----------\n",
            "280 train loss:0.9903 train acc: 0.7430\n",
            "280 val loss:1.0254 val acc: 0.7162\n",
            "训练和验证耗费的时间2m53s\n",
            "Epoch 281/499\n",
            "----------\n",
            "281 train loss:0.9992 train acc: 0.7411\n",
            "281 val loss:1.0213 val acc: 0.7143\n",
            "训练和验证耗费的时间2m54s\n",
            "Epoch 282/499\n",
            "----------\n",
            "282 train loss:0.9897 train acc: 0.7546\n",
            "282 val loss:1.0348 val acc: 0.7085\n",
            "训练和验证耗费的时间2m54s\n",
            "Epoch 283/499\n",
            "----------\n",
            "283 train loss:0.9913 train acc: 0.7498\n",
            "283 val loss:1.0324 val acc: 0.7066\n",
            "训练和验证耗费的时间2m55s\n",
            "Epoch 284/499\n",
            "----------\n",
            "284 train loss:0.9868 train acc: 0.7517\n",
            "284 val loss:1.0336 val acc: 0.6969\n",
            "训练和验证耗费的时间2m56s\n",
            "Epoch 285/499\n",
            "----------\n",
            "285 train loss:0.9893 train acc: 0.7512\n",
            "285 val loss:1.0572 val acc: 0.6795\n",
            "训练和验证耗费的时间2m56s\n",
            "Epoch 286/499\n",
            "----------\n",
            "286 train loss:0.9948 train acc: 0.7445\n",
            "286 val loss:1.0495 val acc: 0.6931\n",
            "训练和验证耗费的时间2m57s\n",
            "Epoch 287/499\n",
            "----------\n",
            "287 train loss:1.0007 train acc: 0.7430\n",
            "287 val loss:1.0361 val acc: 0.7008\n",
            "训练和验证耗费的时间2m57s\n",
            "Epoch 288/499\n",
            "----------\n",
            "288 train loss:0.9973 train acc: 0.7420\n",
            "288 val loss:1.0345 val acc: 0.7085\n",
            "训练和验证耗费的时间2m58s\n",
            "Epoch 289/499\n",
            "----------\n",
            "289 train loss:0.9849 train acc: 0.7594\n",
            "289 val loss:1.0329 val acc: 0.7027\n",
            "训练和验证耗费的时间2m59s\n",
            "Epoch 290/499\n",
            "----------\n",
            "290 train loss:0.9828 train acc: 0.7541\n",
            "290 val loss:1.0291 val acc: 0.7104\n",
            "训练和验证耗费的时间2m59s\n",
            "Epoch 291/499\n",
            "----------\n",
            "291 train loss:0.9969 train acc: 0.7401\n",
            "291 val loss:1.0270 val acc: 0.7008\n",
            "训练和验证耗费的时间2m60s\n",
            "Epoch 292/499\n",
            "----------\n",
            "292 train loss:0.9945 train acc: 0.7488\n",
            "292 val loss:1.0230 val acc: 0.7104\n",
            "训练和验证耗费的时间3m0s\n",
            "Epoch 293/499\n",
            "----------\n",
            "293 train loss:0.9808 train acc: 0.7618\n",
            "293 val loss:1.0411 val acc: 0.6911\n",
            "训练和验证耗费的时间3m1s\n",
            "Epoch 294/499\n",
            "----------\n",
            "294 train loss:1.0014 train acc: 0.7377\n",
            "294 val loss:1.0317 val acc: 0.6988\n",
            "训练和验证耗费的时间3m2s\n",
            "Epoch 295/499\n",
            "----------\n",
            "295 train loss:0.9893 train acc: 0.7507\n",
            "295 val loss:1.0246 val acc: 0.7124\n",
            "训练和验证耗费的时间3m2s\n",
            "Epoch 296/499\n",
            "----------\n",
            "296 train loss:0.9915 train acc: 0.7459\n",
            "296 val loss:1.0265 val acc: 0.7162\n",
            "训练和验证耗费的时间3m3s\n",
            "Epoch 297/499\n",
            "----------\n",
            "297 train loss:0.9913 train acc: 0.7478\n",
            "297 val loss:1.0287 val acc: 0.7162\n",
            "训练和验证耗费的时间3m4s\n",
            "Epoch 298/499\n",
            "----------\n",
            "298 train loss:0.9922 train acc: 0.7473\n",
            "298 val loss:1.0313 val acc: 0.7027\n",
            "训练和验证耗费的时间3m4s\n",
            "Epoch 299/499\n",
            "----------\n",
            "299 train loss:0.9791 train acc: 0.7642\n",
            "299 val loss:1.0225 val acc: 0.7181\n",
            "训练和验证耗费的时间3m5s\n",
            "Epoch 300/499\n",
            "----------\n",
            "300 train loss:0.9807 train acc: 0.7657\n",
            "300 val loss:1.0250 val acc: 0.7085\n",
            "训练和验证耗费的时间3m5s\n",
            "Epoch 301/499\n",
            "----------\n",
            "301 train loss:0.9906 train acc: 0.7488\n",
            "301 val loss:1.0296 val acc: 0.7085\n",
            "训练和验证耗费的时间3m6s\n",
            "Epoch 302/499\n",
            "----------\n",
            "302 train loss:0.9773 train acc: 0.7676\n",
            "302 val loss:1.0300 val acc: 0.7066\n",
            "训练和验证耗费的时间3m7s\n",
            "Epoch 303/499\n",
            "----------\n",
            "303 train loss:0.9924 train acc: 0.7493\n",
            "303 val loss:1.0356 val acc: 0.6988\n",
            "训练和验证耗费的时间3m7s\n",
            "Epoch 304/499\n",
            "----------\n",
            "304 train loss:0.9850 train acc: 0.7536\n",
            "304 val loss:1.0340 val acc: 0.7046\n",
            "训练和验证耗费的时间3m8s\n",
            "Epoch 305/499\n",
            "----------\n",
            "305 train loss:0.9881 train acc: 0.7478\n",
            "305 val loss:1.0267 val acc: 0.7104\n",
            "训练和验证耗费的时间3m8s\n",
            "Epoch 306/499\n",
            "----------\n",
            "306 train loss:0.9916 train acc: 0.7464\n",
            "306 val loss:1.0230 val acc: 0.7124\n",
            "训练和验证耗费的时间3m9s\n",
            "Epoch 307/499\n",
            "----------\n",
            "307 train loss:0.9948 train acc: 0.7440\n",
            "307 val loss:1.0283 val acc: 0.7201\n",
            "训练和验证耗费的时间3m10s\n",
            "Epoch 308/499\n",
            "----------\n",
            "308 train loss:0.9888 train acc: 0.7536\n",
            "308 val loss:1.0290 val acc: 0.7046\n",
            "训练和验证耗费的时间3m10s\n",
            "Epoch 309/499\n",
            "----------\n",
            "309 train loss:0.9893 train acc: 0.7541\n",
            "309 val loss:1.0308 val acc: 0.7027\n",
            "训练和验证耗费的时间3m11s\n",
            "Epoch 310/499\n",
            "----------\n",
            "310 train loss:0.9924 train acc: 0.7454\n",
            "310 val loss:1.0371 val acc: 0.6969\n",
            "训练和验证耗费的时间3m11s\n",
            "Epoch 311/499\n",
            "----------\n",
            "311 train loss:0.9854 train acc: 0.7498\n",
            "311 val loss:1.0342 val acc: 0.6988\n",
            "训练和验证耗费的时间3m12s\n",
            "Epoch 312/499\n",
            "----------\n",
            "312 train loss:0.9819 train acc: 0.7594\n",
            "312 val loss:1.0339 val acc: 0.7008\n",
            "训练和验证耗费的时间3m13s\n",
            "Epoch 313/499\n",
            "----------\n",
            "313 train loss:0.9949 train acc: 0.7440\n",
            "313 val loss:1.0378 val acc: 0.6931\n",
            "训练和验证耗费的时间3m13s\n",
            "Epoch 314/499\n",
            "----------\n",
            "314 train loss:0.9754 train acc: 0.7623\n",
            "314 val loss:1.0286 val acc: 0.7085\n",
            "训练和验证耗费的时间3m14s\n",
            "Epoch 315/499\n",
            "----------\n",
            "315 train loss:0.9840 train acc: 0.7546\n",
            "315 val loss:1.0282 val acc: 0.7124\n",
            "训练和验证耗费的时间3m14s\n",
            "Epoch 316/499\n",
            "----------\n",
            "316 train loss:0.9855 train acc: 0.7551\n",
            "316 val loss:1.0198 val acc: 0.7259\n",
            "训练和验证耗费的时间3m15s\n",
            "Epoch 317/499\n",
            "----------\n",
            "317 train loss:0.9909 train acc: 0.7527\n",
            "317 val loss:1.0283 val acc: 0.7162\n",
            "训练和验证耗费的时间3m16s\n",
            "Epoch 318/499\n",
            "----------\n",
            "318 train loss:0.9847 train acc: 0.7531\n",
            "318 val loss:1.0285 val acc: 0.7162\n",
            "训练和验证耗费的时间3m16s\n",
            "Epoch 319/499\n",
            "----------\n",
            "319 train loss:0.9875 train acc: 0.7531\n",
            "319 val loss:1.0280 val acc: 0.7143\n",
            "训练和验证耗费的时间3m17s\n",
            "Epoch 320/499\n",
            "----------\n",
            "320 train loss:0.9902 train acc: 0.7483\n",
            "320 val loss:1.0356 val acc: 0.6931\n",
            "训练和验证耗费的时间3m18s\n",
            "Epoch 321/499\n",
            "----------\n",
            "321 train loss:0.9804 train acc: 0.7589\n",
            "321 val loss:1.0298 val acc: 0.7027\n",
            "训练和验证耗费的时间3m18s\n",
            "Epoch 322/499\n",
            "----------\n",
            "322 train loss:0.9868 train acc: 0.7551\n",
            "322 val loss:1.0299 val acc: 0.7085\n",
            "训练和验证耗费的时间3m19s\n",
            "Epoch 323/499\n",
            "----------\n",
            "323 train loss:0.9866 train acc: 0.7546\n",
            "323 val loss:1.0319 val acc: 0.7046\n",
            "训练和验证耗费的时间3m19s\n",
            "Epoch 324/499\n",
            "----------\n",
            "324 train loss:0.9780 train acc: 0.7686\n",
            "324 val loss:1.0226 val acc: 0.7239\n",
            "训练和验证耗费的时间3m20s\n",
            "Epoch 325/499\n",
            "----------\n",
            "325 train loss:0.9866 train acc: 0.7502\n",
            "325 val loss:1.0260 val acc: 0.7104\n",
            "训练和验证耗费的时间3m21s\n",
            "Epoch 326/499\n",
            "----------\n",
            "326 train loss:0.9963 train acc: 0.7411\n",
            "326 val loss:1.0290 val acc: 0.7008\n",
            "训练和验证耗费的时间3m21s\n",
            "Epoch 327/499\n",
            "----------\n",
            "327 train loss:0.9797 train acc: 0.7570\n",
            "327 val loss:1.0152 val acc: 0.7239\n",
            "训练和验证耗费的时间3m22s\n",
            "Epoch 328/499\n",
            "----------\n",
            "328 train loss:0.9910 train acc: 0.7464\n",
            "328 val loss:1.0278 val acc: 0.7181\n",
            "训练和验证耗费的时间3m22s\n",
            "Epoch 329/499\n",
            "----------\n",
            "329 train loss:0.9925 train acc: 0.7430\n",
            "329 val loss:1.0193 val acc: 0.7162\n",
            "训练和验证耗费的时间3m23s\n",
            "Epoch 330/499\n",
            "----------\n",
            "330 train loss:0.9987 train acc: 0.7392\n",
            "330 val loss:1.0289 val acc: 0.7104\n",
            "训练和验证耗费的时间3m24s\n",
            "Epoch 331/499\n",
            "----------\n",
            "331 train loss:0.9886 train acc: 0.7522\n",
            "331 val loss:1.0259 val acc: 0.6988\n",
            "训练和验证耗费的时间3m24s\n",
            "Epoch 332/499\n",
            "----------\n",
            "332 train loss:0.9786 train acc: 0.7613\n",
            "332 val loss:1.0255 val acc: 0.7104\n",
            "训练和验证耗费的时间3m25s\n",
            "Epoch 333/499\n",
            "----------\n",
            "333 train loss:0.9929 train acc: 0.7507\n",
            "333 val loss:1.0241 val acc: 0.7143\n",
            "训练和验证耗费的时间3m25s\n",
            "Epoch 334/499\n",
            "----------\n",
            "334 train loss:0.9877 train acc: 0.7565\n",
            "334 val loss:1.0171 val acc: 0.7259\n",
            "训练和验证耗费的时间3m26s\n",
            "Epoch 335/499\n",
            "----------\n",
            "335 train loss:0.9905 train acc: 0.7488\n",
            "335 val loss:1.0283 val acc: 0.7066\n",
            "训练和验证耗费的时间3m27s\n",
            "Epoch 336/499\n",
            "----------\n",
            "336 train loss:0.9940 train acc: 0.7469\n",
            "336 val loss:1.0146 val acc: 0.7278\n",
            "训练和验证耗费的时间3m27s\n",
            "Epoch 337/499\n",
            "----------\n",
            "337 train loss:0.9837 train acc: 0.7565\n",
            "337 val loss:1.0257 val acc: 0.7143\n",
            "训练和验证耗费的时间3m28s\n",
            "Epoch 338/499\n",
            "----------\n",
            "338 train loss:0.9961 train acc: 0.7425\n",
            "338 val loss:1.0225 val acc: 0.7239\n",
            "训练和验证耗费的时间3m29s\n",
            "Epoch 339/499\n",
            "----------\n",
            "339 train loss:0.9827 train acc: 0.7565\n",
            "339 val loss:1.0192 val acc: 0.7259\n",
            "训练和验证耗费的时间3m29s\n",
            "Epoch 340/499\n",
            "----------\n",
            "340 train loss:0.9886 train acc: 0.7493\n",
            "340 val loss:1.0218 val acc: 0.7259\n",
            "训练和验证耗费的时间3m30s\n",
            "Epoch 341/499\n",
            "----------\n",
            "341 train loss:0.9797 train acc: 0.7584\n",
            "341 val loss:1.0265 val acc: 0.7066\n",
            "训练和验证耗费的时间3m30s\n",
            "Epoch 342/499\n",
            "----------\n",
            "342 train loss:0.9755 train acc: 0.7642\n",
            "342 val loss:1.0234 val acc: 0.7143\n",
            "训练和验证耗费的时间3m31s\n",
            "Epoch 343/499\n",
            "----------\n",
            "343 train loss:0.9857 train acc: 0.7599\n",
            "343 val loss:1.0238 val acc: 0.7085\n",
            "训练和验证耗费的时间3m32s\n",
            "Epoch 344/499\n",
            "----------\n",
            "344 train loss:0.9839 train acc: 0.7584\n",
            "344 val loss:1.0201 val acc: 0.7143\n",
            "训练和验证耗费的时间3m32s\n",
            "Epoch 345/499\n",
            "----------\n",
            "345 train loss:0.9889 train acc: 0.7522\n",
            "345 val loss:1.0209 val acc: 0.7143\n",
            "训练和验证耗费的时间3m33s\n",
            "Epoch 346/499\n",
            "----------\n",
            "346 train loss:0.9826 train acc: 0.7565\n",
            "346 val loss:1.0288 val acc: 0.7143\n",
            "训练和验证耗费的时间3m33s\n",
            "Epoch 347/499\n",
            "----------\n",
            "347 train loss:0.9830 train acc: 0.7570\n",
            "347 val loss:1.0240 val acc: 0.7201\n",
            "训练和验证耗费的时间3m34s\n",
            "Epoch 348/499\n",
            "----------\n",
            "348 train loss:0.9853 train acc: 0.7560\n",
            "348 val loss:1.0222 val acc: 0.7201\n",
            "训练和验证耗费的时间3m35s\n",
            "Epoch 349/499\n",
            "----------\n",
            "349 train loss:0.9816 train acc: 0.7594\n",
            "349 val loss:1.0168 val acc: 0.7259\n",
            "训练和验证耗费的时间3m35s\n",
            "Epoch 350/499\n",
            "----------\n",
            "350 train loss:0.9823 train acc: 0.7570\n",
            "350 val loss:1.0244 val acc: 0.7104\n",
            "训练和验证耗费的时间3m36s\n",
            "Epoch 351/499\n",
            "----------\n",
            "351 train loss:0.9792 train acc: 0.7633\n",
            "351 val loss:1.0305 val acc: 0.7027\n",
            "训练和验证耗费的时间3m36s\n",
            "Epoch 352/499\n",
            "----------\n",
            "352 train loss:0.9851 train acc: 0.7560\n",
            "352 val loss:1.0283 val acc: 0.7085\n",
            "训练和验证耗费的时间3m37s\n",
            "Epoch 353/499\n",
            "----------\n",
            "353 train loss:0.9880 train acc: 0.7493\n",
            "353 val loss:1.0466 val acc: 0.6873\n",
            "训练和验证耗费的时间3m38s\n",
            "Epoch 354/499\n",
            "----------\n",
            "354 train loss:0.9913 train acc: 0.7469\n",
            "354 val loss:1.0377 val acc: 0.6969\n",
            "训练和验证耗费的时间3m38s\n",
            "Epoch 355/499\n",
            "----------\n",
            "355 train loss:0.9857 train acc: 0.7541\n",
            "355 val loss:1.0277 val acc: 0.7046\n",
            "训练和验证耗费的时间3m39s\n",
            "Epoch 356/499\n",
            "----------\n",
            "356 train loss:0.9868 train acc: 0.7507\n",
            "356 val loss:1.0318 val acc: 0.6988\n",
            "训练和验证耗费的时间3m39s\n",
            "Epoch 357/499\n",
            "----------\n",
            "357 train loss:0.9854 train acc: 0.7546\n",
            "357 val loss:1.0316 val acc: 0.7066\n",
            "训练和验证耗费的时间3m40s\n",
            "Epoch 358/499\n",
            "----------\n",
            "358 train loss:0.9900 train acc: 0.7522\n",
            "358 val loss:1.0281 val acc: 0.7162\n",
            "训练和验证耗费的时间3m41s\n",
            "Epoch 359/499\n",
            "----------\n",
            "359 train loss:0.9846 train acc: 0.7517\n",
            "359 val loss:1.0218 val acc: 0.7201\n",
            "训练和验证耗费的时间3m41s\n",
            "Epoch 360/499\n",
            "----------\n",
            "360 train loss:0.9730 train acc: 0.7647\n",
            "360 val loss:1.0168 val acc: 0.7259\n",
            "训练和验证耗费的时间3m42s\n",
            "Epoch 361/499\n",
            "----------\n",
            "361 train loss:0.9885 train acc: 0.7469\n",
            "361 val loss:1.0248 val acc: 0.7143\n",
            "训练和验证耗费的时间3m43s\n",
            "Epoch 362/499\n",
            "----------\n",
            "362 train loss:0.9810 train acc: 0.7594\n",
            "362 val loss:1.0312 val acc: 0.6931\n",
            "训练和验证耗费的时间3m43s\n",
            "Epoch 363/499\n",
            "----------\n",
            "363 train loss:0.9786 train acc: 0.7575\n",
            "363 val loss:1.0259 val acc: 0.7162\n",
            "训练和验证耗费的时间3m44s\n",
            "Epoch 364/499\n",
            "----------\n",
            "364 train loss:0.9720 train acc: 0.7724\n",
            "364 val loss:1.0235 val acc: 0.7143\n",
            "训练和验证耗费的时间3m44s\n",
            "Epoch 365/499\n",
            "----------\n",
            "365 train loss:0.9780 train acc: 0.7618\n",
            "365 val loss:1.0288 val acc: 0.7162\n",
            "训练和验证耗费的时间3m45s\n",
            "Epoch 366/499\n",
            "----------\n",
            "366 train loss:0.9749 train acc: 0.7662\n",
            "366 val loss:1.0273 val acc: 0.7162\n",
            "训练和验证耗费的时间3m46s\n",
            "Epoch 367/499\n",
            "----------\n",
            "367 train loss:0.9760 train acc: 0.7676\n",
            "367 val loss:1.0293 val acc: 0.7124\n",
            "训练和验证耗费的时间3m46s\n",
            "Epoch 368/499\n",
            "----------\n",
            "368 train loss:0.9771 train acc: 0.7604\n",
            "368 val loss:1.0296 val acc: 0.7008\n",
            "训练和验证耗费的时间3m47s\n",
            "Epoch 369/499\n",
            "----------\n",
            "369 train loss:0.9698 train acc: 0.7700\n",
            "369 val loss:1.0249 val acc: 0.7162\n",
            "训练和验证耗费的时间3m47s\n",
            "Epoch 370/499\n",
            "----------\n",
            "370 train loss:0.9688 train acc: 0.7748\n",
            "370 val loss:1.0317 val acc: 0.7046\n",
            "训练和验证耗费的时间3m48s\n",
            "Epoch 371/499\n",
            "----------\n",
            "371 train loss:0.9811 train acc: 0.7584\n",
            "371 val loss:1.0267 val acc: 0.7143\n",
            "训练和验证耗费的时间3m49s\n",
            "Epoch 372/499\n",
            "----------\n",
            "372 train loss:0.9866 train acc: 0.7541\n",
            "372 val loss:1.0294 val acc: 0.7104\n",
            "训练和验证耗费的时间3m49s\n",
            "Epoch 373/499\n",
            "----------\n",
            "373 train loss:0.9779 train acc: 0.7671\n",
            "373 val loss:1.0214 val acc: 0.7239\n",
            "训练和验证耗费的时间3m50s\n",
            "Epoch 374/499\n",
            "----------\n",
            "374 train loss:0.9791 train acc: 0.7604\n",
            "374 val loss:1.0413 val acc: 0.6950\n",
            "训练和验证耗费的时间3m50s\n",
            "Epoch 375/499\n",
            "----------\n",
            "375 train loss:0.9843 train acc: 0.7555\n",
            "375 val loss:1.0377 val acc: 0.6931\n",
            "训练和验证耗费的时间3m51s\n",
            "Epoch 376/499\n",
            "----------\n",
            "376 train loss:0.9750 train acc: 0.7633\n",
            "376 val loss:1.0317 val acc: 0.7066\n",
            "训练和验证耗费的时间3m52s\n",
            "Epoch 377/499\n",
            "----------\n",
            "377 train loss:0.9835 train acc: 0.7584\n",
            "377 val loss:1.0273 val acc: 0.7046\n",
            "训练和验证耗费的时间3m52s\n",
            "Epoch 378/499\n",
            "----------\n",
            "378 train loss:0.9752 train acc: 0.7618\n",
            "378 val loss:1.0380 val acc: 0.6873\n",
            "训练和验证耗费的时间3m53s\n",
            "Epoch 379/499\n",
            "----------\n",
            "379 train loss:0.9918 train acc: 0.7396\n",
            "379 val loss:1.0166 val acc: 0.7181\n",
            "训练和验证耗费的时间3m53s\n",
            "Epoch 380/499\n",
            "----------\n",
            "380 train loss:0.9822 train acc: 0.7604\n",
            "380 val loss:1.0192 val acc: 0.7181\n",
            "训练和验证耗费的时间3m54s\n",
            "Epoch 381/499\n",
            "----------\n",
            "381 train loss:0.9663 train acc: 0.7782\n",
            "381 val loss:1.0232 val acc: 0.7181\n",
            "训练和验证耗费的时间3m55s\n",
            "Epoch 382/499\n",
            "----------\n",
            "382 train loss:0.9832 train acc: 0.7580\n",
            "382 val loss:1.0203 val acc: 0.7181\n",
            "训练和验证耗费的时间3m55s\n",
            "Epoch 383/499\n",
            "----------\n",
            "383 train loss:0.9838 train acc: 0.7575\n",
            "383 val loss:1.0232 val acc: 0.7201\n",
            "训练和验证耗费的时间3m56s\n",
            "Epoch 384/499\n",
            "----------\n",
            "384 train loss:0.9685 train acc: 0.7768\n",
            "384 val loss:1.0142 val acc: 0.7317\n",
            "训练和验证耗费的时间3m57s\n",
            "Epoch 385/499\n",
            "----------\n",
            "385 train loss:0.9686 train acc: 0.7715\n",
            "385 val loss:1.0151 val acc: 0.7259\n",
            "训练和验证耗费的时间3m57s\n",
            "Epoch 386/499\n",
            "----------\n",
            "386 train loss:0.9779 train acc: 0.7657\n",
            "386 val loss:1.0241 val acc: 0.7124\n",
            "训练和验证耗费的时间3m58s\n",
            "Epoch 387/499\n",
            "----------\n",
            "387 train loss:0.9831 train acc: 0.7551\n",
            "387 val loss:1.0272 val acc: 0.7066\n",
            "训练和验证耗费的时间3m59s\n",
            "Epoch 388/499\n",
            "----------\n",
            "388 train loss:0.9856 train acc: 0.7555\n",
            "388 val loss:1.0219 val acc: 0.7220\n",
            "训练和验证耗费的时间3m59s\n",
            "Epoch 389/499\n",
            "----------\n",
            "389 train loss:0.9844 train acc: 0.7531\n",
            "389 val loss:1.0256 val acc: 0.7104\n",
            "训练和验证耗费的时间3m60s\n",
            "Epoch 390/499\n",
            "----------\n",
            "390 train loss:0.9760 train acc: 0.7613\n",
            "390 val loss:1.0227 val acc: 0.7008\n",
            "训练和验证耗费的时间4m0s\n",
            "Epoch 391/499\n",
            "----------\n",
            "391 train loss:0.9825 train acc: 0.7565\n",
            "391 val loss:1.0185 val acc: 0.7143\n",
            "训练和验证耗费的时间4m1s\n",
            "Epoch 392/499\n",
            "----------\n",
            "392 train loss:0.9855 train acc: 0.7599\n",
            "392 val loss:1.0251 val acc: 0.7085\n",
            "训练和验证耗费的时间4m2s\n",
            "Epoch 393/499\n",
            "----------\n",
            "393 train loss:0.9704 train acc: 0.7705\n",
            "393 val loss:1.0150 val acc: 0.7181\n",
            "训练和验证耗费的时间4m2s\n",
            "Epoch 394/499\n",
            "----------\n",
            "394 train loss:0.9849 train acc: 0.7555\n",
            "394 val loss:1.0268 val acc: 0.7085\n",
            "训练和验证耗费的时间4m3s\n",
            "Epoch 395/499\n",
            "----------\n",
            "395 train loss:0.9796 train acc: 0.7575\n",
            "395 val loss:1.0206 val acc: 0.7201\n",
            "训练和验证耗费的时间4m3s\n",
            "Epoch 396/499\n",
            "----------\n",
            "396 train loss:0.9734 train acc: 0.7671\n",
            "396 val loss:1.0251 val acc: 0.7104\n",
            "训练和验证耗费的时间4m4s\n",
            "Epoch 397/499\n",
            "----------\n",
            "397 train loss:0.9776 train acc: 0.7584\n",
            "397 val loss:1.0130 val acc: 0.7239\n",
            "训练和验证耗费的时间4m5s\n",
            "Epoch 398/499\n",
            "----------\n",
            "398 train loss:0.9870 train acc: 0.7488\n",
            "398 val loss:1.0164 val acc: 0.7220\n",
            "训练和验证耗费的时间4m5s\n",
            "Epoch 399/499\n",
            "----------\n",
            "399 train loss:0.9751 train acc: 0.7695\n",
            "399 val loss:1.0233 val acc: 0.7201\n",
            "训练和验证耗费的时间4m6s\n",
            "Epoch 400/499\n",
            "----------\n",
            "400 train loss:0.9852 train acc: 0.7599\n",
            "400 val loss:1.0221 val acc: 0.7143\n",
            "训练和验证耗费的时间4m7s\n",
            "Epoch 401/499\n",
            "----------\n",
            "401 train loss:0.9761 train acc: 0.7623\n",
            "401 val loss:1.0294 val acc: 0.7066\n",
            "训练和验证耗费的时间4m7s\n",
            "Epoch 402/499\n",
            "----------\n",
            "402 train loss:0.9764 train acc: 0.7671\n",
            "402 val loss:1.0288 val acc: 0.7027\n",
            "训练和验证耗费的时间4m8s\n",
            "Epoch 403/499\n",
            "----------\n",
            "403 train loss:0.9674 train acc: 0.7772\n",
            "403 val loss:1.0255 val acc: 0.7104\n",
            "训练和验证耗费的时间4m8s\n",
            "Epoch 404/499\n",
            "----------\n",
            "404 train loss:0.9752 train acc: 0.7705\n",
            "404 val loss:1.0230 val acc: 0.7066\n",
            "训练和验证耗费的时间4m9s\n",
            "Epoch 405/499\n",
            "----------\n",
            "405 train loss:0.9926 train acc: 0.7435\n",
            "405 val loss:1.0207 val acc: 0.7124\n",
            "训练和验证耗费的时间4m10s\n",
            "Epoch 406/499\n",
            "----------\n",
            "406 train loss:0.9791 train acc: 0.7560\n",
            "406 val loss:1.0226 val acc: 0.7027\n",
            "训练和验证耗费的时间4m10s\n",
            "Epoch 407/499\n",
            "----------\n",
            "407 train loss:0.9793 train acc: 0.7584\n",
            "407 val loss:1.0326 val acc: 0.7008\n",
            "训练和验证耗费的时间4m11s\n",
            "Epoch 408/499\n",
            "----------\n",
            "408 train loss:0.9831 train acc: 0.7555\n",
            "408 val loss:1.0193 val acc: 0.7181\n",
            "训练和验证耗费的时间4m12s\n",
            "Epoch 409/499\n",
            "----------\n",
            "409 train loss:0.9840 train acc: 0.7565\n",
            "409 val loss:1.0178 val acc: 0.7181\n",
            "训练和验证耗费的时间4m12s\n",
            "Epoch 410/499\n",
            "----------\n",
            "410 train loss:0.9823 train acc: 0.7575\n",
            "410 val loss:1.0251 val acc: 0.7143\n",
            "训练和验证耗费的时间4m13s\n",
            "Epoch 411/499\n",
            "----------\n",
            "411 train loss:0.9851 train acc: 0.7555\n",
            "411 val loss:1.0191 val acc: 0.7143\n",
            "训练和验证耗费的时间4m13s\n",
            "Epoch 412/499\n",
            "----------\n",
            "412 train loss:0.9645 train acc: 0.7768\n",
            "412 val loss:1.0192 val acc: 0.7220\n",
            "训练和验证耗费的时间4m14s\n",
            "Epoch 413/499\n",
            "----------\n",
            "413 train loss:0.9827 train acc: 0.7560\n",
            "413 val loss:1.0233 val acc: 0.7104\n",
            "训练和验证耗费的时间4m15s\n",
            "Epoch 414/499\n",
            "----------\n",
            "414 train loss:0.9795 train acc: 0.7594\n",
            "414 val loss:1.0185 val acc: 0.7143\n",
            "训练和验证耗费的时间4m15s\n",
            "Epoch 415/499\n",
            "----------\n",
            "415 train loss:0.9795 train acc: 0.7647\n",
            "415 val loss:1.0186 val acc: 0.7162\n",
            "训练和验证耗费的时间4m16s\n",
            "Epoch 416/499\n",
            "----------\n",
            "416 train loss:0.9676 train acc: 0.7753\n",
            "416 val loss:1.0272 val acc: 0.7162\n",
            "训练和验证耗费的时间4m16s\n",
            "Epoch 417/499\n",
            "----------\n",
            "417 train loss:0.9741 train acc: 0.7642\n",
            "417 val loss:1.0218 val acc: 0.7085\n",
            "训练和验证耗费的时间4m17s\n",
            "Epoch 418/499\n",
            "----------\n",
            "418 train loss:0.9822 train acc: 0.7604\n",
            "418 val loss:1.0343 val acc: 0.7008\n",
            "训练和验证耗费的时间4m18s\n",
            "Epoch 419/499\n",
            "----------\n",
            "419 train loss:0.9741 train acc: 0.7623\n",
            "419 val loss:1.0240 val acc: 0.7104\n",
            "训练和验证耗费的时间4m18s\n",
            "Epoch 420/499\n",
            "----------\n",
            "420 train loss:0.9655 train acc: 0.7710\n",
            "420 val loss:1.0167 val acc: 0.7201\n",
            "训练和验证耗费的时间4m19s\n",
            "Epoch 421/499\n",
            "----------\n",
            "421 train loss:0.9871 train acc: 0.7536\n",
            "421 val loss:1.0234 val acc: 0.7181\n",
            "训练和验证耗费的时间4m19s\n",
            "Epoch 422/499\n",
            "----------\n",
            "422 train loss:0.9773 train acc: 0.7613\n",
            "422 val loss:1.0193 val acc: 0.7239\n",
            "训练和验证耗费的时间4m20s\n",
            "Epoch 423/499\n",
            "----------\n",
            "423 train loss:0.9700 train acc: 0.7748\n",
            "423 val loss:1.0199 val acc: 0.7104\n",
            "训练和验证耗费的时间4m21s\n",
            "Epoch 424/499\n",
            "----------\n",
            "424 train loss:0.9844 train acc: 0.7536\n",
            "424 val loss:1.0250 val acc: 0.7162\n",
            "训练和验证耗费的时间4m21s\n",
            "Epoch 425/499\n",
            "----------\n",
            "425 train loss:0.9696 train acc: 0.7743\n",
            "425 val loss:1.0273 val acc: 0.7085\n",
            "训练和验证耗费的时间4m22s\n",
            "Epoch 426/499\n",
            "----------\n",
            "426 train loss:0.9759 train acc: 0.7666\n",
            "426 val loss:1.0205 val acc: 0.7124\n",
            "训练和验证耗费的时间4m23s\n",
            "Epoch 427/499\n",
            "----------\n",
            "427 train loss:0.9767 train acc: 0.7642\n",
            "427 val loss:1.0234 val acc: 0.7085\n",
            "训练和验证耗费的时间4m23s\n",
            "Epoch 428/499\n",
            "----------\n",
            "428 train loss:0.9909 train acc: 0.7445\n",
            "428 val loss:1.0219 val acc: 0.7124\n",
            "训练和验证耗费的时间4m24s\n",
            "Epoch 429/499\n",
            "----------\n",
            "429 train loss:0.9798 train acc: 0.7589\n",
            "429 val loss:1.0230 val acc: 0.7085\n",
            "训练和验证耗费的时间4m24s\n",
            "Epoch 430/499\n",
            "----------\n",
            "430 train loss:0.9734 train acc: 0.7662\n",
            "430 val loss:1.0198 val acc: 0.7162\n",
            "训练和验证耗费的时间4m25s\n",
            "Epoch 431/499\n",
            "----------\n",
            "431 train loss:0.9798 train acc: 0.7589\n",
            "431 val loss:1.0203 val acc: 0.7278\n",
            "训练和验证耗费的时间4m26s\n",
            "Epoch 432/499\n",
            "----------\n",
            "432 train loss:0.9777 train acc: 0.7604\n",
            "432 val loss:1.0197 val acc: 0.7143\n",
            "训练和验证耗费的时间4m26s\n",
            "Epoch 433/499\n",
            "----------\n",
            "433 train loss:0.9846 train acc: 0.7555\n",
            "433 val loss:1.0249 val acc: 0.7220\n",
            "训练和验证耗费的时间4m27s\n",
            "Epoch 434/499\n",
            "----------\n",
            "434 train loss:0.9632 train acc: 0.7748\n",
            "434 val loss:1.0253 val acc: 0.7085\n",
            "训练和验证耗费的时间4m27s\n",
            "Epoch 435/499\n",
            "----------\n",
            "435 train loss:0.9757 train acc: 0.7662\n",
            "435 val loss:1.0205 val acc: 0.7143\n",
            "训练和验证耗费的时间4m28s\n",
            "Epoch 436/499\n",
            "----------\n",
            "436 train loss:0.9697 train acc: 0.7700\n",
            "436 val loss:1.0154 val acc: 0.7278\n",
            "训练和验证耗费的时间4m29s\n",
            "Epoch 437/499\n",
            "----------\n",
            "437 train loss:0.9780 train acc: 0.7633\n",
            "437 val loss:1.0159 val acc: 0.7259\n",
            "训练和验证耗费的时间4m29s\n",
            "Epoch 438/499\n",
            "----------\n",
            "438 train loss:0.9723 train acc: 0.7676\n",
            "438 val loss:1.0339 val acc: 0.7027\n",
            "训练和验证耗费的时间4m30s\n",
            "Epoch 439/499\n",
            "----------\n",
            "439 train loss:0.9790 train acc: 0.7589\n",
            "439 val loss:1.0248 val acc: 0.7162\n",
            "训练和验证耗费的时间4m30s\n",
            "Epoch 440/499\n",
            "----------\n",
            "440 train loss:0.9887 train acc: 0.7512\n",
            "440 val loss:1.0203 val acc: 0.7201\n",
            "训练和验证耗费的时间4m31s\n",
            "Epoch 441/499\n",
            "----------\n",
            "441 train loss:0.9840 train acc: 0.7555\n",
            "441 val loss:1.0228 val acc: 0.7181\n",
            "训练和验证耗费的时间4m32s\n",
            "Epoch 442/499\n",
            "----------\n",
            "442 train loss:0.9691 train acc: 0.7724\n",
            "442 val loss:1.0285 val acc: 0.7066\n",
            "训练和验证耗费的时间4m32s\n",
            "Epoch 443/499\n",
            "----------\n",
            "443 train loss:0.9879 train acc: 0.7493\n",
            "443 val loss:1.0278 val acc: 0.7162\n",
            "训练和验证耗费的时间4m33s\n",
            "Epoch 444/499\n",
            "----------\n",
            "444 train loss:0.9689 train acc: 0.7705\n",
            "444 val loss:1.0207 val acc: 0.7278\n",
            "训练和验证耗费的时间4m33s\n",
            "Epoch 445/499\n",
            "----------\n",
            "445 train loss:0.9949 train acc: 0.7425\n",
            "445 val loss:1.0291 val acc: 0.7085\n",
            "训练和验证耗费的时间4m34s\n",
            "Epoch 446/499\n",
            "----------\n",
            "446 train loss:0.9724 train acc: 0.7671\n",
            "446 val loss:1.0336 val acc: 0.6950\n",
            "训练和验证耗费的时间4m35s\n",
            "Epoch 447/499\n",
            "----------\n",
            "447 train loss:0.9771 train acc: 0.7599\n",
            "447 val loss:1.0177 val acc: 0.7239\n",
            "训练和验证耗费的时间4m35s\n",
            "Epoch 448/499\n",
            "----------\n",
            "448 train loss:0.9705 train acc: 0.7700\n",
            "448 val loss:1.0265 val acc: 0.7104\n",
            "训练和验证耗费的时间4m36s\n",
            "Epoch 449/499\n",
            "----------\n",
            "449 train loss:0.9659 train acc: 0.7734\n",
            "449 val loss:1.0292 val acc: 0.6988\n",
            "训练和验证耗费的时间4m37s\n",
            "Epoch 450/499\n",
            "----------\n",
            "450 train loss:0.9757 train acc: 0.7662\n",
            "450 val loss:1.0261 val acc: 0.7143\n",
            "训练和验证耗费的时间4m37s\n",
            "Epoch 451/499\n",
            "----------\n",
            "451 train loss:0.9845 train acc: 0.7555\n",
            "451 val loss:1.0218 val acc: 0.7085\n",
            "训练和验证耗费的时间4m38s\n",
            "Epoch 452/499\n",
            "----------\n",
            "452 train loss:0.9735 train acc: 0.7652\n",
            "452 val loss:1.0209 val acc: 0.7162\n",
            "训练和验证耗费的时间4m38s\n",
            "Epoch 453/499\n",
            "----------\n",
            "453 train loss:0.9640 train acc: 0.7787\n",
            "453 val loss:1.0206 val acc: 0.7181\n",
            "训练和验证耗费的时间4m39s\n",
            "Epoch 454/499\n",
            "----------\n",
            "454 train loss:0.9777 train acc: 0.7589\n",
            "454 val loss:1.0174 val acc: 0.7162\n",
            "训练和验证耗费的时间4m40s\n",
            "Epoch 455/499\n",
            "----------\n",
            "455 train loss:0.9700 train acc: 0.7763\n",
            "455 val loss:1.0254 val acc: 0.7124\n",
            "训练和验证耗费的时间4m40s\n",
            "Epoch 456/499\n",
            "----------\n",
            "456 train loss:0.9679 train acc: 0.7748\n",
            "456 val loss:1.0207 val acc: 0.7201\n",
            "训练和验证耗费的时间4m41s\n",
            "Epoch 457/499\n",
            "----------\n",
            "457 train loss:0.9754 train acc: 0.7608\n",
            "457 val loss:1.0235 val acc: 0.7124\n",
            "训练和验证耗费的时间4m41s\n",
            "Epoch 458/499\n",
            "----------\n",
            "458 train loss:0.9776 train acc: 0.7633\n",
            "458 val loss:1.0231 val acc: 0.7124\n",
            "训练和验证耗费的时间4m42s\n",
            "Epoch 459/499\n",
            "----------\n",
            "459 train loss:0.9727 train acc: 0.7652\n",
            "459 val loss:1.0171 val acc: 0.7239\n",
            "训练和验证耗费的时间4m43s\n",
            "Epoch 460/499\n",
            "----------\n",
            "460 train loss:0.9753 train acc: 0.7652\n",
            "460 val loss:1.0270 val acc: 0.7027\n",
            "训练和验证耗费的时间4m43s\n",
            "Epoch 461/499\n",
            "----------\n",
            "461 train loss:0.9778 train acc: 0.7633\n",
            "461 val loss:1.0249 val acc: 0.7143\n",
            "训练和验证耗费的时间4m44s\n",
            "Epoch 462/499\n",
            "----------\n",
            "462 train loss:0.9720 train acc: 0.7681\n",
            "462 val loss:1.0225 val acc: 0.7239\n",
            "训练和验证耗费的时间4m44s\n",
            "Epoch 463/499\n",
            "----------\n",
            "463 train loss:0.9744 train acc: 0.7642\n",
            "463 val loss:1.0315 val acc: 0.6950\n",
            "训练和验证耗费的时间4m45s\n",
            "Epoch 464/499\n",
            "----------\n",
            "464 train loss:0.9723 train acc: 0.7676\n",
            "464 val loss:1.0160 val acc: 0.7220\n",
            "训练和验证耗费的时间4m46s\n",
            "Epoch 465/499\n",
            "----------\n",
            "465 train loss:0.9743 train acc: 0.7676\n",
            "465 val loss:1.0182 val acc: 0.7162\n",
            "训练和验证耗费的时间4m46s\n",
            "Epoch 466/499\n",
            "----------\n",
            "466 train loss:0.9789 train acc: 0.7594\n",
            "466 val loss:1.0113 val acc: 0.7317\n",
            "训练和验证耗费的时间4m47s\n",
            "Epoch 467/499\n",
            "----------\n",
            "467 train loss:0.9750 train acc: 0.7628\n",
            "467 val loss:1.0267 val acc: 0.7066\n",
            "训练和验证耗费的时间4m48s\n",
            "Epoch 468/499\n",
            "----------\n",
            "468 train loss:0.9646 train acc: 0.7797\n",
            "468 val loss:1.0205 val acc: 0.7181\n",
            "训练和验证耗费的时间4m48s\n",
            "Epoch 469/499\n",
            "----------\n",
            "469 train loss:0.9706 train acc: 0.7700\n",
            "469 val loss:1.0312 val acc: 0.7027\n",
            "训练和验证耗费的时间4m49s\n",
            "Epoch 470/499\n",
            "----------\n",
            "470 train loss:0.9732 train acc: 0.7695\n",
            "470 val loss:1.0171 val acc: 0.7220\n",
            "训练和验证耗费的时间4m49s\n",
            "Epoch 471/499\n",
            "----------\n",
            "471 train loss:0.9718 train acc: 0.7700\n",
            "471 val loss:1.0142 val acc: 0.7317\n",
            "训练和验证耗费的时间4m50s\n",
            "Epoch 472/499\n",
            "----------\n",
            "472 train loss:0.9722 train acc: 0.7686\n",
            "472 val loss:1.0143 val acc: 0.7239\n",
            "训练和验证耗费的时间4m51s\n",
            "Epoch 473/499\n",
            "----------\n",
            "473 train loss:0.9810 train acc: 0.7584\n",
            "473 val loss:1.0207 val acc: 0.7259\n",
            "训练和验证耗费的时间4m51s\n",
            "Epoch 474/499\n",
            "----------\n",
            "474 train loss:0.9661 train acc: 0.7772\n",
            "474 val loss:1.0212 val acc: 0.7162\n",
            "训练和验证耗费的时间4m52s\n",
            "Epoch 475/499\n",
            "----------\n",
            "475 train loss:0.9822 train acc: 0.7594\n",
            "475 val loss:1.0195 val acc: 0.7259\n",
            "训练和验证耗费的时间4m52s\n",
            "Epoch 476/499\n",
            "----------\n",
            "476 train loss:0.9717 train acc: 0.7671\n",
            "476 val loss:1.0185 val acc: 0.7181\n",
            "训练和验证耗费的时间4m53s\n",
            "Epoch 477/499\n",
            "----------\n",
            "477 train loss:0.9621 train acc: 0.7816\n",
            "477 val loss:1.0172 val acc: 0.7239\n",
            "训练和验证耗费的时间4m54s\n",
            "Epoch 478/499\n",
            "----------\n",
            "478 train loss:0.9679 train acc: 0.7739\n",
            "478 val loss:1.0167 val acc: 0.7278\n",
            "训练和验证耗费的时间4m54s\n",
            "Epoch 479/499\n",
            "----------\n",
            "479 train loss:0.9731 train acc: 0.7686\n",
            "479 val loss:1.0222 val acc: 0.7143\n",
            "训练和验证耗费的时间4m55s\n",
            "Epoch 480/499\n",
            "----------\n",
            "480 train loss:0.9737 train acc: 0.7719\n",
            "480 val loss:1.0148 val acc: 0.7336\n",
            "训练和验证耗费的时间4m55s\n",
            "Epoch 481/499\n",
            "----------\n",
            "481 train loss:0.9714 train acc: 0.7734\n",
            "481 val loss:1.0291 val acc: 0.7085\n",
            "训练和验证耗费的时间4m56s\n",
            "Epoch 482/499\n",
            "----------\n",
            "482 train loss:0.9780 train acc: 0.7657\n",
            "482 val loss:1.0227 val acc: 0.7239\n",
            "训练和验证耗费的时间4m57s\n",
            "Epoch 483/499\n",
            "----------\n",
            "483 train loss:0.9659 train acc: 0.7772\n",
            "483 val loss:1.0170 val acc: 0.7220\n",
            "训练和验证耗费的时间4m57s\n",
            "Epoch 484/499\n",
            "----------\n",
            "484 train loss:0.9620 train acc: 0.7792\n",
            "484 val loss:1.0179 val acc: 0.7143\n",
            "训练和验证耗费的时间4m58s\n",
            "Epoch 485/499\n",
            "----------\n",
            "485 train loss:0.9666 train acc: 0.7768\n",
            "485 val loss:1.0221 val acc: 0.7143\n",
            "训练和验证耗费的时间4m59s\n",
            "Epoch 486/499\n",
            "----------\n",
            "486 train loss:0.9704 train acc: 0.7686\n",
            "486 val loss:1.0134 val acc: 0.7181\n",
            "训练和验证耗费的时间4m59s\n",
            "Epoch 487/499\n",
            "----------\n",
            "487 train loss:0.9662 train acc: 0.7724\n",
            "487 val loss:1.0129 val acc: 0.7239\n",
            "训练和验证耗费的时间4m60s\n",
            "Epoch 488/499\n",
            "----------\n",
            "488 train loss:0.9639 train acc: 0.7787\n",
            "488 val loss:1.0255 val acc: 0.6988\n",
            "训练和验证耗费的时间5m0s\n",
            "Epoch 489/499\n",
            "----------\n",
            "489 train loss:0.9693 train acc: 0.7676\n",
            "489 val loss:1.0205 val acc: 0.7085\n",
            "训练和验证耗费的时间5m1s\n",
            "Epoch 490/499\n",
            "----------\n",
            "490 train loss:0.9723 train acc: 0.7695\n",
            "490 val loss:1.0340 val acc: 0.6950\n",
            "训练和验证耗费的时间5m2s\n",
            "Epoch 491/499\n",
            "----------\n",
            "491 train loss:0.9634 train acc: 0.7811\n",
            "491 val loss:1.0239 val acc: 0.7201\n",
            "训练和验证耗费的时间5m2s\n",
            "Epoch 492/499\n",
            "----------\n",
            "492 train loss:0.9726 train acc: 0.7676\n",
            "492 val loss:1.0099 val acc: 0.7201\n",
            "训练和验证耗费的时间5m3s\n",
            "Epoch 493/499\n",
            "----------\n",
            "493 train loss:0.9775 train acc: 0.7647\n",
            "493 val loss:1.0250 val acc: 0.7104\n",
            "训练和验证耗费的时间5m3s\n",
            "Epoch 494/499\n",
            "----------\n",
            "494 train loss:0.9659 train acc: 0.7748\n",
            "494 val loss:1.0146 val acc: 0.7181\n",
            "训练和验证耗费的时间5m4s\n",
            "Epoch 495/499\n",
            "----------\n",
            "495 train loss:0.9722 train acc: 0.7671\n",
            "495 val loss:1.0120 val acc: 0.7239\n",
            "训练和验证耗费的时间5m5s\n",
            "Epoch 496/499\n",
            "----------\n",
            "496 train loss:0.9665 train acc: 0.7710\n",
            "496 val loss:1.0202 val acc: 0.7162\n",
            "训练和验证耗费的时间5m5s\n",
            "Epoch 497/499\n",
            "----------\n",
            "497 train loss:0.9572 train acc: 0.7845\n",
            "497 val loss:1.0125 val acc: 0.7220\n",
            "训练和验证耗费的时间5m6s\n",
            "Epoch 498/499\n",
            "----------\n",
            "498 train loss:0.9579 train acc: 0.7850\n",
            "498 val loss:1.0301 val acc: 0.7027\n",
            "训练和验证耗费的时间5m6s\n",
            "Epoch 499/499\n",
            "----------\n",
            "499 train loss:0.9695 train acc: 0.7686\n",
            "499 val loss:1.0227 val acc: 0.7104\n",
            "训练和验证耗费的时间5m7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model_test.py**"
      ],
      "metadata": {
        "id": "U02guqzH4SWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "import mne\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "#from model import EEGNet\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "\n",
        "# 1、创建必要的本地目录，用于保存数据\n",
        "if not os.path.exists('2a_test_pre'):\n",
        "    os.makedirs('2a_test_pre')\n",
        "\n",
        "# 2、原始数据读取和通道重命名\n",
        "data_path = ['A0'+str(i)+'E' for i in range(1, 10)]\n",
        "raw = [mne.io.read_raw_gdf(input_fname='./drive/MyDrive/'+path+'.gdf',\n",
        "                           stim_channel=\"auto\",\n",
        "                           preload=True,\n",
        "                           verbose='error') for path in data_path]\n",
        "\n",
        "for i in range(len(raw)):\n",
        "    raw[i].rename_channels({'EEG-Fz': 'Fz', 'EEG-0': 'FC3', 'EEG-1': 'FC1', 'EEG-2': 'FCz', 'EEG-3': 'FC2',\n",
        "                            'EEG-4': 'FC4', 'EEG-5': 'C5', 'EEG-C3': 'C3', 'EEG-6': 'C1', 'EEG-Cz': 'Cz',\n",
        "                            'EEG-7': 'C2', 'EEG-C4': 'C4', 'EEG-8': 'C6', 'EEG-9': 'CP3', 'EEG-10': 'CP1',\n",
        "                            'EEG-11': 'CPz', 'EEG-12': 'CP2', 'EEG-13': 'CP4', 'EEG-14': 'P1', 'EEG-15': 'Pz',\n",
        "                            'EEG-16': 'P2', 'EEG-Pz': 'POz'})\n",
        "\n",
        "# 3、提取MI时间，完成坏值清洗，并封装\n",
        "events = []\n",
        "for i in range(len(raw)):\n",
        "    event, event_dict = mne.events_from_annotations(raw[i])\n",
        "    events.append(event)\n",
        "\n",
        "    # 打印事件信息用于调试\n",
        "    print(f\"文件 {data_path[i]} 的事件类型: {event_dict}\")\n",
        "\n",
        "    raw[i].load_data()\n",
        "    data = raw[i].get_data()\n",
        "    for i_chan in range(data.shape[0]):\n",
        "        chan = data[i_chan]\n",
        "        data[i_chan] = np.where(chan == np.min(chan), np.nan, chan)\n",
        "        mask = np.isnan(data[i_chan])\n",
        "        chan_mean = np.nanmean(data[i_chan])\n",
        "        data[i_chan, mask] = chan_mean\n",
        "    raw[i] = mne.io.RawArray(data, raw[i].info, verbose=\"error\")\n",
        "\n",
        "# 4、切段、去EOG、做标准化，封存数据为npz\n",
        "tmin, tmax = 0, 4\n",
        "for i in range(len(raw)):\n",
        "    # 对于测试数据，使用768事件（cue开始）来创建epochs\n",
        "    # 在测试数据中，768事件对应MI任务的开始\n",
        "    event_id_test = {'768': 6}  # 使用768事件\n",
        "\n",
        "    try:\n",
        "        epochs = mne.Epochs(raw[i], events[i], event_id=event_id_test, tmin=tmin, tmax=tmax,\n",
        "                           proj=False, baseline=None, preload=True)\n",
        "    except Exception as e:\n",
        "        print(f\"创建epochs失败: {e}\")\n",
        "        print(f\"事件类型: {np.unique(events[i][:, 2])}\")\n",
        "        continue\n",
        "\n",
        "    exclude = [\"EOG-left\", \"EOG-central\", \"EOG-right\"]\n",
        "    epochs.drop_channels(exclude)\n",
        "\n",
        "    labels_file = scipy.io.loadmat('./drive/MyDrive/'+data_path[i]+'.mat')\n",
        "\n",
        "    # 检查测试数据中是否有classlabel\n",
        "    if 'classlabel' in labels_file:\n",
        "        labels = labels_file['classlabel'].reshape(-1)\n",
        "        print(f\"文件 {data_path[i]} 使用classlabel，标签分布: {np.unique(labels, return_counts=True)}\")\n",
        "    else:\n",
        "        # 对于测试数据，我们需要从其他来源获取标签\n",
        "        print(f\"文件 {data_path[i]} 没有classlabel，需要从其他来源获取标签\")\n",
        "\n",
        "        # 方法1: 检查是否有true_labels\n",
        "        if 'true_labels' in labels_file:\n",
        "            labels = labels_file['true_labels'].reshape(-1)\n",
        "            print(f\"使用true_labels，标签分布: {np.unique(labels, return_counts=True)}\")\n",
        "        # 方法2: 检查是否有其他标签字段\n",
        "        elif 'label' in labels_file:\n",
        "            labels = labels_file['label'].reshape(-1)\n",
        "            print(f\"使用label，标签分布: {np.unique(labels, return_counts=True)}\")\n",
        "        else:\n",
        "            # 方法3: 对于真正的测试数据，我们可能需要使用伪标签或从其他文件获取\n",
        "            print(f\"警告: 无法找到标签信息，使用均衡伪标签\")\n",
        "            # 创建均衡的伪标签 (1,2,3,4)\n",
        "            n_epochs = len(epochs)\n",
        "            labels = np.array([1, 2, 3, 4] * (n_epochs // 4 + 1))[:n_epochs]\n",
        "            print(f\"使用伪标签，标签分布: {np.unique(labels, return_counts=True)}\")\n",
        "\n",
        "    epochs_data = epochs.get_data(copy=True)\n",
        "\n",
        "    # 检查是否需要调整时间点\n",
        "    if epochs_data.shape[2] > 1000:\n",
        "        epochs_data = epochs_data[:, :, :1000]  # 取前1000个时间点\n",
        "    elif epochs_data.shape[2] < 1000:\n",
        "        # 如果时间点不足，进行填充\n",
        "        padded_data = np.zeros((epochs_data.shape[0], epochs_data.shape[1], 1000))\n",
        "        padded_data[:, :, :epochs_data.shape[2]] = epochs_data\n",
        "        epochs_data = padded_data\n",
        "\n",
        "    n_samples, n_channels, n_timepoints = epochs_data.shape\n",
        "    epochs_data_flat = epochs_data.reshape(n_samples, -1)\n",
        "\n",
        "    scaler = StandardScaler().fit(epochs_data_flat)\n",
        "    data_scaled = scaler.transform(epochs_data_flat)\n",
        "\n",
        "    data_scaled = data_scaled.reshape(n_samples, n_channels, n_timepoints)\n",
        "\n",
        "    # 确保数据和标签的数量一致\n",
        "    min_length = min(len(data_scaled), len(labels))\n",
        "    data_scaled = data_scaled[:min_length]\n",
        "    labels = labels[:min_length]\n",
        "\n",
        "    print(f\"文件 {data_path[i]}: 数据形状 {data_scaled.shape}, 标签形状 {labels.shape}\")\n",
        "\n",
        "    np.savez('2a_test_pre/'+data_path[i]+'.npz', data=data_scaled, label=labels)\n",
        "\n",
        "\n",
        "# 5、创建测试集数据加载器\n",
        "def create_simple_dataloaders():\n",
        "    # 加载数据\n",
        "    x_test, y_test = [], []\n",
        "    for i in range(1, 10):\n",
        "        try:\n",
        "            test_data = np.load(f'2a_test_pre/A0{i}E.npz')\n",
        "            x_test.append(test_data['data'])\n",
        "            y_test.append(test_data['label'])\n",
        "            print(f\"加载 A0{i}E.npz: 数据形状 {test_data['data'].shape}, 标签形状 {test_data['label'].shape}\")\n",
        "            print(f\"标签分布: {np.unique(test_data['label'], return_counts=True)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"加载 A0{i}E.npz 时出错: {e}\")\n",
        "            continue\n",
        "\n",
        "    # 检查是否成功加载了数据\n",
        "    if len(x_test) == 0:\n",
        "        raise ValueError(\"没有成功加载任何测试数据\")\n",
        "\n",
        "    # 合并数据\n",
        "    x_test = np.concatenate(x_test)\n",
        "    y_test = np.concatenate(y_test)\n",
        "\n",
        "    print(f\"合并后 - 数据形状: {x_test.shape}, 标签形状: {y_test.shape}\")\n",
        "    print(f\"合并后标签分布: {np.unique(y_test, return_counts=True)}\")\n",
        "\n",
        "    # 转换为PyTorch张量\n",
        "    x_test = torch.FloatTensor(x_test).unsqueeze(1)\n",
        "    y_test = torch.LongTensor(y_test - 1)  # 标签从1-4转换为0-3\n",
        "\n",
        "    print(f\"转换后标签范围: {torch.unique(y_test)}\")\n",
        "\n",
        "    # 创建DataLoader\n",
        "    test_loader = DataLoader(\n",
        "        TensorDataset(x_test, y_test),\n",
        "        batch_size=32,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "# 6、模型测试\n",
        "def test_model_process(model, test_loader):\n",
        "    # 设定测试所用到的设备，有GPU用GPU没有GPU用CPU\n",
        "    device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"使用设备: {device}\")\n",
        "\n",
        "    # 将模型放入到训练设备中\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 打印模型结构\n",
        "    print(\"模型结构:\")\n",
        "    print(model)\n",
        "\n",
        "    # 初始化参数\n",
        "    test_corrects = 0.0\n",
        "    test_num = 0\n",
        "\n",
        "    # 存储预测结果用于分析\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # 只进行前向传播计算，不计算梯度，从而节省内存，加快运行速度\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (test_data_x, test_data_y) in enumerate(test_loader):\n",
        "            # 将特征放入到测试设备中\n",
        "            test_data_x = test_data_x.to(device)\n",
        "            # 将标签放入到测试设备中\n",
        "            test_data_y = test_data_y.to(device)\n",
        "            # 设置模型为评估模式\n",
        "            model.eval()\n",
        "            # 前向传播过程，输入为测试数据集，输出为对每个样本的预测值\n",
        "            output = model(test_data_x)\n",
        "\n",
        "            # 打印第一个batch的输出用于调试\n",
        "            if batch_idx == 0:\n",
        "                print(f\"模型输出形状: {output.shape}\")\n",
        "                print(f\"前5个样本的输出: {output[:5]}\")\n",
        "                print(f\"前5个样本的softmax概率: {torch.softmax(output[:5], dim=1)}\")\n",
        "\n",
        "            # 查找每一行中最大值对应的行标\n",
        "            pre_lab = torch.argmax(output, dim=1)\n",
        "\n",
        "            # 存储预测结果和真实标签\n",
        "            all_predictions.extend(pre_lab.cpu().numpy())\n",
        "            all_labels.extend(test_data_y.cpu().numpy())\n",
        "\n",
        "            # 如果预测正确，则准确度test_corrects加1\n",
        "            test_corrects += torch.sum(pre_lab == test_data_y.data)\n",
        "            # 将所有的测试样本进行累加\n",
        "            test_num += test_data_x.size(0)\n",
        "\n",
        "            # 打印第一个batch的预测结果\n",
        "            if batch_idx == 0:\n",
        "                print(f\"第一个batch的预测: {pre_lab[:10]}\")\n",
        "                print(f\"第一个batch的真实标签: {test_data_y[:10]}\")\n",
        "                print(f\"第一个batch的正确数量: {torch.sum(pre_lab == test_data_y.data)}\")\n",
        "\n",
        "    # 计算测试准确率\n",
        "    test_acc = test_corrects.double().item() / test_num\n",
        "\n",
        "    # 分析预测结果\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    print(f\"\\n预测结果分析:\")\n",
        "    print(f\"预测标签分布: {np.unique(all_predictions, return_counts=True)}\")\n",
        "    print(f\"真实标签分布: {np.unique(all_labels, return_counts=True)}\")\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix, classification_report\n",
        "    print(f\"\\n混淆矩阵:\")\n",
        "    print(confusion_matrix(all_labels, all_predictions))\n",
        "    print(f\"\\n分类报告:\")\n",
        "    print(classification_report(all_labels, all_predictions))\n",
        "\n",
        "    print(f\"\\n测试的准确率为: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "# 7、模型开始测试\n",
        "if __name__ == \"__main__\":\n",
        "    # 检查模型文件是否存在\n",
        "    if not os.path.exists('best_model.pth'):\n",
        "        print(\"错误: 找不到模型文件 'best_model.pth'\")\n",
        "        print(\"请确保模型文件存在于当前目录\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"加载模型...\")\n",
        "    model = EEGNet()\n",
        "\n",
        "    try:\n",
        "        # 尝试使用weights_only=True加载\n",
        "        model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
        "        print(\"模型加载成功 (weights_only=True)\")\n",
        "    except:\n",
        "        try:\n",
        "            # 如果失败，尝试使用weights_only=False\n",
        "            model.load_state_dict(torch.load('best_model.pth', weights_only=False))\n",
        "            print(\"模型加载成功 (weights_only=False)\")\n",
        "        except Exception as e:\n",
        "            print(f\"模型加载失败: {e}\")\n",
        "            exit(1)\n",
        "\n",
        "    print(\"创建数据加载器...\")\n",
        "    test_loader = create_simple_dataloaders()\n",
        "\n",
        "    print(\"开始测试...\")\n",
        "    test_model_process(model, test_loader)"
      ],
      "metadata": {
        "id": "OqfBuPsq3p2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "outputId": "73d6ffab-f34c-44bd-c150-c0fa97be9e51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A01E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A02E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A03E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A04E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A05E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A06E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A07E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A08E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Used Annotations descriptions: [np.str_('1023'), np.str_('1072'), np.str_('276'), np.str_('277'), np.str_('32766'), np.str_('768'), np.str_('783')]\n",
            "文件 A09E 的事件类型: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('783'): 7}\n",
            "Not setting metadata\n",
            "288 matching events found\n",
            "No baseline correction applied\n",
            "Using data from preloaded Raw for 288 events and 1001 original time points ...\n",
            "0 bad epochs dropped\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './drive/MyDriveA01E.mat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/MyDriveA01E.mat'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2468167094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mlabels_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drive/MyDrive'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# 检查测试数据中是否有classlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[1;32m    232\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise OSError(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/MyDriveA01E.mat'"
          ]
        }
      ]
    }
  ]
}